{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {}
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers torch_geometric"
      ],
      "metadata": {
        "id": "syG7jbCnev-O",
        "outputId": "09316197-2b4e-42d7-f0d3-2d4221005877",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T04:27:05.147491Z",
          "iopub.execute_input": "2025-03-31T04:27:05.147800Z",
          "iopub.status.idle": "2025-03-31T04:27:10.528973Z",
          "shell.execute_reply.started": "2025-03-31T04:27:05.147774Z",
          "shell.execute_reply": "2025-03-31T04:27:10.527990Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m52.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load the CNN/DailyMail dataset\n",
        "cnn_dataset = datasets.load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# Convert to pandas DataFrame for easier manipulation\n",
        "train_df = pd.DataFrame(cnn_dataset[\"train\"])\n",
        "val_df = pd.DataFrame(cnn_dataset[\"validation\"])\n",
        "test_df = pd.DataFrame(cnn_dataset[\"test\"])\n",
        "\n",
        "# Sample 1% of the training data\n",
        "sample_size = int(len(train_df) * 0.001)\n",
        "train_sample = train_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"Full training set size: {len(train_df)}\")\n",
        "print(f\"1% sample size: {len(train_sample)}\")\n"
      ],
      "metadata": {
        "id": "GQZrcNRSetap",
        "outputId": "f87eb8d0-a668-41cb-edf7-2db4ff76e3da",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T09:26:14.095412Z",
          "iopub.execute_input": "2025-03-31T09:26:14.095710Z",
          "iopub.status.idle": "2025-03-31T09:26:28.114546Z",
          "shell.execute_reply.started": "2025-03-31T09:26:14.095679Z",
          "shell.execute_reply": "2025-03-31T09:26:28.113717Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Full training set size: 287113\n1% sample size: 287\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def tokenize_document(document):\n",
        "    # Split document into sentences\n",
        "    sentences = sent_tokenize(document)\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove empty sentences\n",
        "    processed_sentences = [s for s in processed_sentences if s.strip()]\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "# Apply preprocessing to the sampled data\n",
        "train_sample['processed_article'] = train_sample['article'].apply(tokenize_document)\n",
        "train_sample['processed_highlights'] = train_sample['highlights'].apply(tokenize_document)\n",
        "\n",
        "# Create labels for extractive summarization (1 if sentence is in highlights, 0 otherwise)\n",
        "def create_extractive_labels(article_sentences, highlight_sentences):\n",
        "    labels = []\n",
        "    for sentence in article_sentences:\n",
        "        # Check if this sentence is similar to any highlight sentence\n",
        "        is_in_highlights = any(\n",
        "            similarity_score(sentence, highlight) > 0.7\n",
        "            for highlight in highlight_sentences\n",
        "        )\n",
        "        labels.append(1 if is_in_highlights else 0)\n",
        "    return labels\n",
        "\n",
        "def similarity_score(sent1, sent2):\n",
        "    # Simple word overlap similarity\n",
        "    words1 = set(word_tokenize(sent1))\n",
        "    words2 = set(word_tokenize(sent2))\n",
        "\n",
        "    if not words1 or not words2:\n",
        "        return 0\n",
        "\n",
        "    overlap = len(words1.intersection(words2))\n",
        "    return overlap / max(len(words1), len(words2))\n",
        "\n",
        "# Create extractive labels\n",
        "train_sample['extractive_labels'] = [\n",
        "    create_extractive_labels(article, highlight)\n",
        "    for article, highlight in zip(train_sample['processed_article'], train_sample['processed_highlights'])\n",
        "]\n"
      ],
      "metadata": {
        "id": "4YlrvDCqeLAk",
        "outputId": "d5086db9-8870-4f98-92f8-c36fa8c26023",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T09:26:28.115604Z",
          "iopub.execute_input": "2025-03-31T09:26:28.115935Z",
          "iopub.status.idle": "2025-03-31T09:26:37.280004Z",
          "shell.execute_reply.started": "2025-03-31T09:26:28.115911Z",
          "shell.execute_reply": "2025-03-31T09:26:37.279241Z"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Package punkt_tab is already up-to-date!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "class TextGraph:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tfidf = TfidfVectorizer()\n",
        "\n",
        "    def build_graph(self, document_sentences):\n",
        "        # Extract all unique words\n",
        "        all_words = set()\n",
        "        for sentence in document_sentences:\n",
        "            words = [word for word in word_tokenize(sentence) if word not in stopwords.words('english')]\n",
        "            all_words.update(words)\n",
        "\n",
        "        # Create node feature vectors\n",
        "        sentence_features = self._encode_sentences(document_sentences)\n",
        "        word_features = self._encode_words(list(all_words))\n",
        "\n",
        "        # Calculate TF-IDF for edge weights\n",
        "        self.tfidf.fit(document_sentences)\n",
        "        tfidf_matrix = self.tfidf.transform(document_sentences)\n",
        "\n",
        "        # Create edges (word-sentence connections)\n",
        "        edges = []\n",
        "        edge_weights = []\n",
        "\n",
        "        for word_idx, word in enumerate(all_words):\n",
        "            for sent_idx, sentence in enumerate(document_sentences):\n",
        "                if word in word_tokenize(sentence):\n",
        "                    # Word to sentence edge\n",
        "                    edges.append([word_idx, sent_idx + len(all_words)])\n",
        "\n",
        "                    # Get TF-IDF weight\n",
        "                    word_id = self.tfidf.vocabulary_.get(word, -1)\n",
        "                    weight = tfidf_matrix[sent_idx, word_id] if word_id != -1 else 0\n",
        "                    edge_weights.append(weight)\n",
        "\n",
        "        return {\n",
        "            'sentence_features': sentence_features,\n",
        "            'word_features': word_features,\n",
        "            'edges': torch.tensor(edges).t().contiguous() if edges else torch.zeros((2, 0), dtype=torch.long),\n",
        "            'edge_weights': torch.tensor(edge_weights) if edge_weights else torch.zeros(0),\n",
        "            'sentences': document_sentences,\n",
        "            'num_sentences': len(document_sentences),\n",
        "            'num_words': len(all_words)\n",
        "        }\n",
        "\n",
        "    def _encode_sentences(self, sentences):\n",
        "        # Encode sentences using BERT\n",
        "        with torch.no_grad():\n",
        "            encoded_sentences = []\n",
        "            for sentence in sentences:\n",
        "                inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "                outputs = self.bert(**inputs)\n",
        "                # Use CLS token as sentence representation\n",
        "                encoded_sentences.append(outputs.last_hidden_state[:, 0, :].squeeze())\n",
        "\n",
        "            if encoded_sentences:\n",
        "                return torch.stack(encoded_sentences)\n",
        "            else:\n",
        "                return torch.zeros((0, 768))\n",
        "\n",
        "    def _encode_words(self, words):\n",
        "        # Encode words using BERT\n",
        "        with torch.no_grad():\n",
        "            encoded_words = []\n",
        "            for word in words:\n",
        "                inputs = self.tokenizer(word, return_tensors='pt')\n",
        "                outputs = self.bert(**inputs)\n",
        "                # Use mean of token embeddings as word representation\n",
        "                encoded_words.append(outputs.last_hidden_state.mean(dim=1).squeeze())\n",
        "\n",
        "            if encoded_words:\n",
        "                return torch.stack(encoded_words)\n",
        "            else:\n",
        "                return torch.zeros((0, 768))\n"
      ],
      "metadata": {
        "id": "QmHVRnaFgPTG",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T04:28:38.224660Z",
          "iopub.execute_input": "2025-03-31T04:28:38.225342Z",
          "iopub.status.idle": "2025-03-31T04:28:38.235458Z",
          "shell.execute_reply.started": "2025-03-31T04:28:38.225310Z",
          "shell.execute_reply": "2025-03-31T04:28:38.234565Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class SummarizationGNN(nn.Module):\n",
        "    def __init__(self, word_dim=768, sentence_dim=768, hidden_dim=256, num_heads=8):\n",
        "        super(SummarizationGNN, self).__init__()\n",
        "\n",
        "        # Word and sentence projections to the same dimension\n",
        "        self.word_projection = nn.Linear(word_dim, hidden_dim)\n",
        "        self.sentence_projection = nn.Linear(sentence_dim, hidden_dim)\n",
        "\n",
        "        # GAT layers\n",
        "        self.word_to_sentence = GATConv(hidden_dim, hidden_dim, heads=num_heads, concat=False)\n",
        "\n",
        "        # Feed-forward networks after GAT\n",
        "        self.sentence_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Sentence classifier\n",
        "        self.sentence_classifier = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, graph):\n",
        "        # Get features and structure\n",
        "        word_features = graph['word_features']\n",
        "        sentence_features = graph['sentence_features']\n",
        "        edges = graph['edges']\n",
        "        edge_weights = graph['edge_weights']\n",
        "        num_words = graph['num_words']\n",
        "\n",
        "        # Handle empty graphs\n",
        "        if word_features.size(0) == 0 or sentence_features.size(0) == 0 or edges.size(1) == 0:\n",
        "            return torch.zeros(sentence_features.size(0))\n",
        "\n",
        "        # Project features to the same dimension\n",
        "        word_features = self.word_projection(word_features)\n",
        "        sentence_features = self.sentence_projection(sentence_features)\n",
        "\n",
        "        # Combine features\n",
        "        all_features = torch.cat([word_features, sentence_features], dim=0)\n",
        "\n",
        "        # Word to sentence message passing\n",
        "        updated_features = self.word_to_sentence(all_features, edges, edge_weights)\n",
        "\n",
        "        # Apply FFN to sentence features\n",
        "        sentence_updated = self.sentence_ffn(updated_features[num_words:])\n",
        "\n",
        "        # Sentence classification\n",
        "        sentence_scores = self.sentence_classifier(sentence_updated).squeeze(-1)\n",
        "\n",
        "        return sentence_scores\n"
      ],
      "metadata": {
        "id": "Ey3Ya7yxf5Bd",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T04:28:45.795626Z",
          "iopub.execute_input": "2025-03-31T04:28:45.795948Z",
          "iopub.status.idle": "2025-03-31T04:28:47.029427Z",
          "shell.execute_reply.started": "2025-03-31T04:28:45.795922Z",
          "shell.execute_reply": "2025-03-31T04:28:47.028759Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class GraphDataset(Dataset):\n",
        "    def __init__(self, dataframe, text_graph):\n",
        "        self.dataframe = dataframe\n",
        "        self.text_graph = text_graph\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article_sentences = self.dataframe.iloc[idx]['processed_article']\n",
        "        labels = self.dataframe.iloc[idx]['extractive_labels']\n",
        "\n",
        "        # Build graph\n",
        "        graph = self.text_graph.build_graph(article_sentences)\n",
        "\n",
        "        # Convert labels to tensor and move to device\n",
        "        graph['labels'] = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "        return graph\n",
        "\n",
        "def collate_graphs(batch):\n",
        "    return batch\n",
        "\n",
        "# Initialize text graph builder\n",
        "text_graph = TextGraph()\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = GraphDataset(train_sample, text_graph)\n",
        "\n",
        "# Create dataloader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,  # Process one document at a time\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_graphs\n",
        ")\n",
        "\n",
        "# Initialize model and move it to GPU\n",
        "model = SummarizationGNN().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 3\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        graph = batch[0]  # Get the single graph from the batch\n",
        "\n",
        "        # Skip empty graphs\n",
        "        if graph['sentence_features'].size(0) == 0 or graph['word_features'].size(0) == 0 or graph['edges'].size(1) == 0:\n",
        "            continue\n",
        "\n",
        "        # Move all tensors to GPU\n",
        "        graph['sentence_features'] = graph['sentence_features'].to(device)\n",
        "        graph['word_features'] = graph['word_features'].to(device)\n",
        "        graph['edges'] = graph['edges'].to(device)\n",
        "        if 'edge_weights' in graph:\n",
        "            graph['edge_weights'] = graph['edge_weights'].to(device)  # Move edge weights to GPU\n",
        "        graph['labels'] = graph['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        scores = model(graph)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, graph['labels'])\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"gnn_summarizer_model.pt\")\n"
      ],
      "metadata": {
        "id": "Hz1AIDxjf7LH",
        "outputId": "76d19545-e79e-442c-eb58-5fa97e4c9a10",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T04:28:59.391416Z",
          "iopub.execute_input": "2025-03-31T04:28:59.391720Z",
          "iopub.status.idle": "2025-03-31T06:56:20.618856Z",
          "shell.execute_reply.started": "2025-03-31T04:28:59.391694Z",
          "shell.execute_reply": "2025-03-31T06:56:20.618013Z"
        },
        "colab": {
          "referenced_widgets": [
            "7ba824179a0d4a5e886a9e9a8334b317",
            "6aa58af035554f9b96b5f5f5a99c80d3",
            "81b32135e53f4cbeb3a0cd50d00a342c",
            "6ba912ba77724f0e906751aa0e1ee872",
            "9d4d43e49b8c459c8bda23941ab52ccc"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ba824179a0d4a5e886a9e9a8334b317"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6aa58af035554f9b96b5f5f5a99c80d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "81b32135e53f4cbeb3a0cd50d00a342c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ba912ba77724f0e906751aa0e1ee872"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d4d43e49b8c459c8bda23941ab52ccc"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Epoch 1/3: 100%|██████████| 287/287 [48:04<00:00, 10.05s/it] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/3, Loss: 0.0856\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/3: 100%|██████████| 287/287 [49:37<00:00, 10.37s/it] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/3, Loss: 0.0654\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 3/3: 100%|██████████| 287/287 [49:34<00:00, 10.36s/it]",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 3/3, Loss: 0.0817\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_summary(article, model, text_graph, ratio=0.3):\n",
        "    device = next(model.parameters()).device  # Get model's device\n",
        "\n",
        "    # Preprocess article\n",
        "    sentences = tokenize_document(article)\n",
        "\n",
        "    # Build graph\n",
        "    graph = text_graph.build_graph(sentences)\n",
        "\n",
        "    # Skip empty graphs\n",
        "    if graph['sentence_features'].size(0) == 0 or graph['word_features'].size(0) == 0 or graph['edges'].size(1) == 0:\n",
        "        return \"\"  # Return empty summary if the graph is empty\n",
        "\n",
        "    # Move all tensors in graph to the model's device\n",
        "    graph['sentence_features'] = graph['sentence_features'].to(device)\n",
        "    graph['word_features'] = graph['word_features'].to(device)\n",
        "    graph['edges'] = graph['edges'].to(device)\n",
        "    if 'edge_weights' in graph:\n",
        "        graph['edge_weights'] = graph['edge_weights'].to(device)\n",
        "\n",
        "    # Get sentence scores\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sentence_scores = model(graph)\n",
        "\n",
        "    # Select top sentences\n",
        "    num_sentences = graph['num_sentences']\n",
        "    num_to_select = max(1, int(num_sentences * ratio))\n",
        "\n",
        "    # Get indices of top sentences\n",
        "    _, indices = torch.topk(sentence_scores, min(num_to_select, len(sentence_scores)))\n",
        "    selected_indices = sorted(indices.tolist())\n",
        "\n",
        "    # Generate summary\n",
        "    original_sentences = sent_tokenize(article)\n",
        "    summary_sentences = [original_sentences[i] for i in selected_indices if i < len(original_sentences)]\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Test the model on a sample article\n",
        "sample_article = test_df.iloc[0]['article']\n",
        "generated_summary = generate_summary(sample_article, model, text_graph)\n",
        "actual_summary = test_df.iloc[0]['highlights']\n",
        "\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)\n",
        "print(\"\\nActual Summary:\")\n",
        "print(actual_summary)\n"
      ],
      "metadata": {
        "id": "sz17jAvdf-pY",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T06:56:20.620003Z",
          "iopub.execute_input": "2025-03-31T06:56:20.620300Z",
          "iopub.status.idle": "2025-03-31T06:56:29.182360Z",
          "shell.execute_reply.started": "2025-03-31T06:56:20.620276Z",
          "shell.execute_reply": "2025-03-31T06:56:29.181614Z"
        },
        "outputId": "73f210b3-6254-4745-e0ca-a0626db5f3fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generated Summary:\nThe formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. Rights group Human Rights Watch welcomed the development. The United States also said it \"strongly\" disagreed with the court's decision. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes.\n\nActual Summary:\nMembership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\nIsrael and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T06:56:29.184028Z",
          "iopub.execute_input": "2025-03-31T06:56:29.184324Z",
          "iopub.status.idle": "2025-03-31T06:56:34.514875Z",
          "shell.execute_reply.started": "2025-03-31T06:56:29.184301Z",
          "shell.execute_reply": "2025-03-31T06:56:34.513982Z"
        },
        "id": "pTYr1vspmv6g",
        "outputId": "92f4be37-ccb8-46f6-b6bc-0073d0128e45"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge-score) (2024.2.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=7c345d4869e35035f4c2881f1964c62bcfb4cb4a01017d2b4250b7651123b383\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def evaluate_summaries(generated_summaries, reference_summaries):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "    for gen_sum, ref_sum in zip(generated_summaries, reference_summaries):\n",
        "        score = scorer.score(ref_sum, gen_sum)\n",
        "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
        "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
        "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
        "\n",
        "    # Compute average scores\n",
        "    avg_scores = {key: sum(values) / len(values) if values else 0.0 for key, values in scores.items()}\n",
        "    return avg_scores\n",
        "\n",
        "# Generate summaries for a small test set\n",
        "test_sample = test_df.head(10)\n",
        "generated_summaries = []\n",
        "reference_summaries = []\n",
        "\n",
        "for _, row in test_sample.iterrows():\n",
        "    generated_summary = generate_summary(row['article'], model, text_graph)\n",
        "    generated_summaries.append(generated_summary if generated_summary else \" \")  # Avoid empty strings\n",
        "    reference_summaries.append(row['highlights'])\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_scores = evaluate_summaries(generated_summaries, reference_summaries)\n",
        "\n",
        "# Display ROUGE scores\n",
        "print(\"ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n"
      ],
      "metadata": {
        "id": "Q1-OsTcCgAuj",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T06:56:34.516725Z",
          "iopub.execute_input": "2025-03-31T06:56:34.517032Z",
          "iopub.status.idle": "2025-03-31T06:57:41.252688Z",
          "shell.execute_reply.started": "2025-03-31T06:56:34.517004Z",
          "shell.execute_reply": "2025-03-31T06:57:41.251946Z"
        },
        "outputId": "881bf372-910b-4fec-b147-dc6df8f133db"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ROUGE Scores:\nROUGE-1: 0.2843\nROUGE-2: 0.1019\nROUGE-L: 0.1971\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
        "# Load tokenizer\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T06:57:41.253512Z",
          "iopub.execute_input": "2025-03-31T06:57:41.253739Z",
          "iopub.status.idle": "2025-03-31T06:57:47.461457Z",
          "shell.execute_reply.started": "2025-03-31T06:57:41.253708Z",
          "shell.execute_reply": "2025-03-31T06:57:47.460732Z"
        },
        "colab": {
          "referenced_widgets": [
            "82865130b6dd4c56b197b7274325e584",
            "510b981ef956402c8197a4c73f139b1e",
            "6210044dd04146039eb45a9534efced3",
            "87ad7e88524f46409087243e304e5348"
          ]
        },
        "id": "0xDEpAiqmv6h",
        "outputId": "0a5b4e62-1b57-4dce-96bb-28d2644b20da"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82865130b6dd4c56b197b7274325e584"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "510b981ef956402c8197a4c73f139b1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6210044dd04146039eb45a9534efced3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "87ad7e88524f46409087243e304e5348"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Generate GNN-based summaries\n",
        "train_sample['gnn_summary'] = train_sample['article'].apply(lambda x: generate_summary(x, model, text_graph))\n",
        "\n",
        "# Split into train and validation sets (90% train, 10% validation)\n",
        "train_df, val_df = train_test_split(train_sample, test_size=0.1, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T09:42:10.395319Z",
          "iopub.execute_input": "2025-03-31T09:42:10.395620Z"
        },
        "id": "lJoPUqD7mv6h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset,DatasetDict\n",
        "\n",
        "# Convert Pandas DataFrames to Hugging Face Dataset\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict({\n",
        "        \"input_text\": train_df[\"gnn_summary\"].tolist(),\n",
        "        \"target_text\": train_df[\"highlights\"].tolist(),\n",
        "    }),\n",
        "    \"validation\": Dataset.from_dict({\n",
        "        \"input_text\": val_df[\"gnn_summary\"].tolist(),\n",
        "        \"target_text\": val_df[\"highlights\"].tolist(),\n",
        "    })\n",
        "})\n",
        "\n",
        "# Extract train and validation datasets\n",
        "train_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"validation\"]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T08:27:44.751446Z",
          "iopub.execute_input": "2025-03-31T08:27:44.751740Z",
          "iopub.status.idle": "2025-03-31T08:27:44.768456Z",
          "shell.execute_reply.started": "2025-03-31T08:27:44.751719Z",
          "shell.execute_reply": "2025-03-31T08:27:44.767678Z"
        },
        "id": "vMJqryS3mv6h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenization function\n",
        "def tokenize_function(batch):\n",
        "    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    targets = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Load pre-trained BART model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T08:27:49.321013Z",
          "iopub.execute_input": "2025-03-31T08:27:49.321330Z",
          "iopub.status.idle": "2025-03-31T08:27:55.855315Z",
          "shell.execute_reply.started": "2025-03-31T08:27:49.321305Z",
          "shell.execute_reply": "2025-03-31T08:27:55.854685Z"
        },
        "colab": {
          "referenced_widgets": [
            "0c0f9e4541af4bdaa183ca7d70386859",
            "af4bce336b83435bbbf8e85d0f91fd2a",
            "846c249a1e1d4e909673dc36b947d0b3",
            "c8999c505a644b58825d6867d6c5232a"
          ]
        },
        "id": "EztT4e6Cmv6h",
        "outputId": "965bed99-18dc-435c-9ad7-19226930ad69"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/258 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c0f9e4541af4bdaa183ca7d70386859"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/29 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af4bce336b83435bbbf8e85d0f91fd2a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.63G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "846c249a1e1d4e909673dc36b947d0b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8999c505a644b58825d6867d6c5232a"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")\n",
        "wandb.login(key=secret_value_0)\n",
        "wandb.init(project=\"Transformer + GNN\")\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./bart_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",  # Enables evaluation every epoch\n",
        "    save_strategy=\"epoch\",        # Saves model every epoch\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    save_total_limit=0,  # Limits saved checkpoints to avoid storage issues\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,  # Include validation set\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T09:35:40.351759Z",
          "iopub.execute_input": "2025-03-31T09:35:40.352072Z",
          "iopub.status.idle": "2025-03-31T09:38:41.719319Z",
          "shell.execute_reply.started": "2025-03-31T09:35:40.352046Z",
          "shell.execute_reply": "2025-03-31T09:38:41.718635Z"
        },
        "id": "gHSoD8JImv6i",
        "outputId": "e094e9bc-4ac7-46ab-cd23-88302f542f32"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb22cs093\u001b[0m (\u001b[33mb22cs093-prom-iit-rajasthan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.19.1"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20250331_093541-603nltv3</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/Transformer%20%2B%20GNN/runs/603nltv3' target=\"_blank\">toasty-music-1</a></strong> to <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/Transformer%20%2B%20GNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/Transformer%20%2B%20GNN' target=\"_blank\">https://wandb.ai/b22cs093-prom-iit-rajasthan/Transformer%20%2B%20GNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/Transformer%20%2B%20GNN/runs/603nltv3' target=\"_blank\">https://wandb.ai/b22cs093-prom-iit-rajasthan/Transformer%20%2B%20GNN/runs/603nltv3</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='195' max='195' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [195/195 02:51, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>No log</td>\n      <td>1.274772</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>No log</td>\n      <td>1.225530</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>No log</td>\n      <td>1.262935</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 142, 'min_length': 56, 'early_stopping': True, 'num_beams': 4, 'length_penalty': 2.0, 'no_repeat_ngram_size': 3, 'forced_bos_token_id': 0}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n  warnings.warn(\nThere were missing keys in the checkpoint model loaded: ['model.encoder.embed_tokens.weight', 'model.decoder.embed_tokens.weight', 'lm_head.weight'].\n",
          "output_type": "stream"
        },
        {
          "execution_count": 32,
          "output_type": "execute_result",
          "data": {
            "text/plain": "TrainOutput(global_step=195, training_loss=1.3796649639423078, metrics={'train_runtime': 172.6718, 'train_samples_per_second': 4.482, 'train_steps_per_second': 1.129, 'total_flos': 838669481017344.0, 'train_loss': 1.3796649639423078, 'epoch': 3.0})"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"bart_finetuned\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T09:00:02.917503Z",
          "iopub.status.idle": "2025-03-31T09:00:02.917797Z",
          "shell.execute_reply": "2025-03-31T09:00:02.917688Z"
        },
        "id": "hDd3fzpEmv6i"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Ensure model is in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# ROUGE Evaluation Function\n",
        "def evaluate_summaries(generated_summaries, reference_summaries):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "    for gen_sum, ref_sum in zip(generated_summaries, reference_summaries):\n",
        "        score = scorer.score(ref_sum, gen_sum)\n",
        "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
        "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
        "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
        "\n",
        "    avg_scores = {key: sum(values) / len(values) if values else 0.0 for key, values in scores.items()}\n",
        "    return avg_scores\n",
        "\n",
        "# Generate summaries for a small test set\n",
        "test_sample = val_df.head(10)  # Using validation set\n",
        "generated_summaries = []\n",
        "reference_summaries = []\n",
        "\n",
        "with torch.no_grad():  # Disable gradient tracking for inference\n",
        "    for _, row in test_sample.iterrows():\n",
        "        input_text = row['gnn_summary']\n",
        "\n",
        "        if not input_text or not isinstance(input_text, str):  # Handle empty/missing summaries\n",
        "            generated_summaries.append(\" \")\n",
        "            reference_summaries.append(row['highlights'])\n",
        "            continue\n",
        "\n",
        "        input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=512).input_ids.to(model.device)\n",
        "        output_ids = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n",
        "        generated_summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "        generated_summaries.append(generated_summary)\n",
        "        reference_summaries.append(row['highlights'])\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_scores = evaluate_summaries(generated_summaries, reference_summaries)\n",
        "\n",
        "# Display ROUGE scores\n",
        "print(\"\\nROUGE Scores (Fine-tuned BART on GNN Summaries):\")\n",
        "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T09:04:05.717540Z",
          "iopub.execute_input": "2025-03-31T09:04:05.717850Z",
          "iopub.status.idle": "2025-03-31T09:04:14.274311Z",
          "shell.execute_reply.started": "2025-03-31T09:04:05.717828Z",
          "shell.execute_reply": "2025-03-31T09:04:14.273642Z"
        },
        "outputId": "d705aa29-2468-4c8d-ddf2-38f0f96fdd9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kC9r1_P9p7vg"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ROUGE Scores (Fine-tuned BART on GNN Summaries):\n",
            "ROUGE-1: 0.4520\n",
            "ROUGE-2: 0.2314\n",
            "ROUGE-L: 0.3117\n"
          ]
        }
      ],
      "execution_count": null
    }
  ]
}