{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f7b0c30ac1df4e9bad93499b55d38de6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91691b82efe6438c8d7183d34377176f",
              "IPY_MODEL_4a0004ce6fff4745a69ef06d717d6176",
              "IPY_MODEL_f2b416d4c25e4280b0752c91e586a981"
            ],
            "layout": "IPY_MODEL_bf14afb4c3ca421d8261ea1e286b94ca"
          }
        },
        "91691b82efe6438c8d7183d34377176f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3aed9cc89d647288d1a9487ce113cac",
            "placeholder": "​",
            "style": "IPY_MODEL_5f0034762f3a496d94323c04d1fa11f7",
            "value": "Map: 100%"
          }
        },
        "4a0004ce6fff4745a69ef06d717d6176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be0b10d0bab04a4c8f78ef0df44dcb49",
            "max": 258,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e02df059938945629effd5c6b2cccb04",
            "value": 258
          }
        },
        "f2b416d4c25e4280b0752c91e586a981": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7504f55c9246475fba256e59db72d965",
            "placeholder": "​",
            "style": "IPY_MODEL_e409d052cdcc48e28dd0b6e807783f61",
            "value": " 258/258 [00:00&lt;00:00, 280.88 examples/s]"
          }
        },
        "bf14afb4c3ca421d8261ea1e286b94ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3aed9cc89d647288d1a9487ce113cac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0034762f3a496d94323c04d1fa11f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be0b10d0bab04a4c8f78ef0df44dcb49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e02df059938945629effd5c6b2cccb04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7504f55c9246475fba256e59db72d965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e409d052cdcc48e28dd0b6e807783f61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6a624739adb45afbca9f8ae298a0d6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_57e4b8ea46364cd4845283bf7fbeebc9",
              "IPY_MODEL_2e2425cc551c451e838a09370ff01ee9",
              "IPY_MODEL_e0e46f7b7d504b7c8cdb2ee86051cc0f"
            ],
            "layout": "IPY_MODEL_6c9731993c1048c7b0e72b34d43106c9"
          }
        },
        "57e4b8ea46364cd4845283bf7fbeebc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e893660331641599d52157947e3cc72",
            "placeholder": "​",
            "style": "IPY_MODEL_0fce2ac7193540e2bceb9636ee9a2a12",
            "value": "Map: 100%"
          }
        },
        "2e2425cc551c451e838a09370ff01ee9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_319b69fefb284a4ab11d6bacd394ac6e",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_385f3fa4c2f8418b96e4b1c122afbe3e",
            "value": 29
          }
        },
        "e0e46f7b7d504b7c8cdb2ee86051cc0f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db50ad442e5842c8bb27deffac895c5b",
            "placeholder": "​",
            "style": "IPY_MODEL_bb2dab403ae84eceb7a1d69e77453fdd",
            "value": " 29/29 [00:00&lt;00:00, 191.26 examples/s]"
          }
        },
        "6c9731993c1048c7b0e72b34d43106c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e893660331641599d52157947e3cc72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0fce2ac7193540e2bceb9636ee9a2a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "319b69fefb284a4ab11d6bacd394ac6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "385f3fa4c2f8418b96e4b1c122afbe3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "db50ad442e5842c8bb27deffac895c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb2dab403ae84eceb7a1d69e77453fdd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVg7X1QcbIXU",
        "outputId": "d95cfc20-e728-4f7e-c9cd-a71f1f8f260d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.29.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'datasets', 'transformers','wandb'])\n",
        "\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"bilstm-summarization\", name=\"bilstm-seq2seq\")\n",
        "\n",
        "# Load the CNN/DailyMail dataset\n",
        "cnn_dataset = datasets.load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# Convert to pandas DataFrame for easier manipulation\n",
        "train_df = pd.DataFrame(cnn_dataset[\"train\"])\n",
        "val_df = pd.DataFrame(cnn_dataset[\"validation\"])\n",
        "test_df = pd.DataFrame(cnn_dataset[\"test\"])\n",
        "\n",
        "# Sample a smaller portion of the training data for faster processing\n",
        "sample_size = int(len(train_df) * 0.001)\n",
        "train_sample = train_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"Full training set size: {len(train_df)}\")\n",
        "print(f\"Sample size: {len(train_sample)}\")\n",
        "\n",
        "# Log dataset info to wandb\n",
        "wandb.config.update({\n",
        "    \"dataset\": \"CNN/DailyMail\",\n",
        "    \"full_train_size\": len(train_df),\n",
        "    \"sample_size\": len(train_sample),\n",
        "    \"val_size\": len(val_df),\n",
        "    \"test_size\": len(test_df)\n",
        "})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T10:45:25.537194Z",
          "iopub.execute_input": "2025-03-31T10:45:25.537473Z",
          "iopub.status.idle": "2025-03-31T10:46:00.421297Z",
          "shell.execute_reply.started": "2025-03-31T10:45:25.537438Z",
          "shell.execute_reply": "2025-03-31T10:46:00.420338Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "NNfeFn3kZeG6",
        "outputId": "a400cfbc-bb51-4ecf-daea-c67581193c05"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.8"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250331_142643-ywnqerul</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/bilstm-summarization/runs/ywnqerul' target=\"_blank\">bilstm-seq2seq</a></strong> to <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/bilstm-summarization' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/bilstm-summarization' target=\"_blank\">https://wandb.ai/b22cs093-prom-iit-rajasthan/bilstm-summarization</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/bilstm-summarization/runs/ywnqerul' target=\"_blank\">https://wandb.ai/b22cs093-prom-iit-rajasthan/bilstm-summarization/runs/ywnqerul</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Full training set size: 287113\n",
            "Sample size: 287\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def tokenize_document(document):\n",
        "    # Split document into sentences\n",
        "    sentences = sent_tokenize(document)\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove empty sentences\n",
        "    processed_sentences = [s for s in processed_sentences if s.strip()]\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "# Apply preprocessing to the sampled data\n",
        "train_sample['processed_article'] = train_sample['article'].apply(tokenize_document)\n",
        "train_sample['processed_highlights'] = train_sample['highlights'].apply(tokenize_document)\n",
        "\n",
        "# Create labels for extractive summarization (1 if sentence is in highlights, 0 otherwise)\n",
        "def create_extractive_labels(article_sentences, highlight_sentences):\n",
        "    labels = []\n",
        "    for sentence in article_sentences:\n",
        "        # Check if this sentence is similar to any highlight sentence\n",
        "        is_in_highlights = any(\n",
        "            similarity_score(sentence, highlight) > 0.7\n",
        "            for highlight in highlight_sentences\n",
        "        )\n",
        "        labels.append(1 if is_in_highlights else 0)\n",
        "    return labels\n",
        "\n",
        "def similarity_score(sent1, sent2):\n",
        "    # Simple word overlap similarity\n",
        "    words1 = set(word_tokenize(sent1))\n",
        "    words2 = set(word_tokenize(sent2))\n",
        "\n",
        "    if not words1 or not words2:\n",
        "        return 0\n",
        "\n",
        "    overlap = len(words1.intersection(words2))\n",
        "    return overlap / max(len(words1), len(words2))\n",
        "\n",
        "# Create extractive labels\n",
        "train_sample['extractive_labels'] = [\n",
        "    create_extractive_labels(article, highlight)\n",
        "    for article, highlight in zip(train_sample['processed_article'], train_sample['processed_highlights'])\n",
        "]\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T10:46:00.423205Z",
          "iopub.execute_input": "2025-03-31T10:46:00.423673Z",
          "iopub.status.idle": "2025-03-31T10:46:15.071384Z",
          "shell.execute_reply.started": "2025-03-31T10:46:00.423635Z",
          "shell.execute_reply": "2025-03-31T10:46:15.070484Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5oWozweZeG7",
        "outputId": "c50e9312-c048-4c49-f8b6-630ae87d8512"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8V_cLgWZeG-",
        "outputId": "f4017975-3a52-4d0e-b986-196f39f23811"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge-score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge-score) (4.67.1)\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback,TrainerCallback\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cKmIU96ZeG-",
        "outputId": "5f75f04e-5c23-4940-a690-358b0b335767"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define BiLSTM model architecture\n",
        "class BiLSTMSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Encoder (BiLSTM)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                             bidirectional=True, batch_first=True, num_layers=num_layers,\n",
        "                             dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "        # Decoder (LSTM with attention)\n",
        "        self.decoder = nn.LSTM(embedding_dim + hidden_dim*2, hidden_dim*2,  # Attention concatenation\n",
        "                             batch_first=True, dropout=dropout, num_layers=1)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_dim*2 + hidden_dim*2, hidden_dim*2)\n",
        "        self.v = nn.Linear(hidden_dim*2, 1, bias=False)\n",
        "\n",
        "        # Final projection layer\n",
        "        self.fc = nn.Linear(hidden_dim*2, vocab_size)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, trg=None, max_len=128, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        # Encoder Forward Pass\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embedded)\n",
        "\n",
        "        # Prepare decoder initial states\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "        # Decoder Setup\n",
        "        if trg is None:\n",
        "            trg = torch.zeros((batch_size, max_len), dtype=torch.long, device=src.device)\n",
        "            trg[:,0] = 1  # Start with SOS token\n",
        "\n",
        "        decoder_input = self.embedding(trg[:,0].unsqueeze(1))\n",
        "        outputs = torch.zeros(max_len, batch_size, self.fc.out_features, device=src.device)\n",
        "\n",
        "        # Decoding Loop\n",
        "        for t in range(1, max_len):\n",
        "            # Attention Calculation\n",
        "            energy = torch.tanh(self.attention(torch.cat((\n",
        "                hidden.repeat(encoder_outputs.size(1), 1, 1).permute(1,0,2),\n",
        "                encoder_outputs\n",
        "            ), dim=2)))\n",
        "\n",
        "            attention = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
        "            context = torch.bmm(attention.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "            # Decoder Step\n",
        "            decoder_output, (hidden, cell) = self.decoder(\n",
        "                torch.cat((decoder_input, context), dim=2),\n",
        "                (hidden, cell)\n",
        "            )\n",
        "\n",
        "            # Project to vocabulary space\n",
        "            output = self.fc(decoder_output.squeeze(1))\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Teacher Forcing\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            decoder_input = self.embedding(trg[:,t].unsqueeze(1) if use_teacher_forcing else top1.unsqueeze(1))\n",
        "            decoder_input = self.dropout(decoder_input)\n",
        "\n",
        "        return outputs.permute(1, 0, 2)\n",
        "\n",
        "    def generate(self, src, max_len=128, temperature=1.0):\n",
        "        with torch.no_grad():\n",
        "            # Encoder forward pass\n",
        "            encoder_outputs, (hidden, cell) = self.encoder(self.embedding(src))\n",
        "\n",
        "            # Prepare decoder initial states\n",
        "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "            cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "            outputs = []\n",
        "            decoder_input = torch.tensor([[1]], device=src.device)  # SOS token\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                decoder_emb = self.embedding(decoder_input)\n",
        "\n",
        "                # Attention\n",
        "                energy = torch.tanh(self.attention(torch.cat((\n",
        "                    hidden.repeat(encoder_outputs.size(1), 1, 1).permute(1,0,2),\n",
        "                    encoder_outputs\n",
        "                ), dim=2)))\n",
        "\n",
        "                attention = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
        "                context = torch.bmm(attention.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "                # Decoder step\n",
        "                decoder_output, (hidden, cell) = self.decoder(\n",
        "                    torch.cat((decoder_emb, context), dim=2),\n",
        "                    (hidden, cell)\n",
        "                )\n",
        "\n",
        "                # Output projection\n",
        "                logits = self.fc(decoder_output.squeeze(1)) / temperature\n",
        "                probabilities = F.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probabilities, 1)\n",
        "\n",
        "                if next_token.item() == 2:  # EOS token\n",
        "                    break\n",
        "\n",
        "                outputs.append(next_token.item())\n",
        "                decoder_input = next_token\n",
        "\n",
        "            return outputs"
      ],
      "metadata": {
        "trusted": true,
        "id": "AW0vXYY1ZeG_"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a wrapper model compatible with HuggingFace Trainer\n",
        "class BiLSTMWrapper(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.model = base_model\n",
        "\n",
        "    def forward(self, input_ids=None, labels=None, attention_mask=None, **kwargs):\n",
        "        # Forward pass through the model\n",
        "        if labels is not None:\n",
        "            # Training mode with labels\n",
        "            outputs = self.model(src=input_ids, trg=labels)\n",
        "\n",
        "            # Calculate loss - CrossEntropyLoss\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = 0\n",
        "\n",
        "            # Calculate loss for each position in the sequence\n",
        "            for t in range(1, outputs.size(1)):\n",
        "                loss += loss_fct(outputs[:, t, :], labels[:, t])\n",
        "\n",
        "            # Average loss across positions\n",
        "            loss = loss / (outputs.size(1) - 1)\n",
        "\n",
        "            return {\"loss\": loss, \"logits\": outputs}\n",
        "        else:\n",
        "            # Inference mode\n",
        "            return {\"logits\": self.model(src=input_ids)}"
      ],
      "metadata": {
        "id": "Z-EhXNxPhd5X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and validation sets\n",
        "train_df, val_df = train_test_split(train_sample, test_size=0.1, random_state=42)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Convert Pandas DataFrames to Hugging Face Dataset\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict({\n",
        "        \"input_text\": train_df[\"article\"].tolist(),\n",
        "        \"target_text\": train_df[\"highlights\"].tolist(),\n",
        "    }),\n",
        "    \"validation\": Dataset.from_dict({\n",
        "        \"input_text\": val_df[\"article\"].tolist(),\n",
        "        \"target_text\": val_df[\"highlights\"].tolist(),\n",
        "    })\n",
        "})\n",
        "\n",
        "# Extract train and validation datasets\n",
        "train_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"validation\"]\n",
        "\n",
        "# Define tokenization function\n",
        "def tokenize_function(batch):\n",
        "    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    targets = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Initialize the BiLSTM model\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "base_model = BiLSTMSeq2Seq(len(tokenizer), embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout).to(device)\n",
        "model = BiLSTMWrapper(base_model)\n",
        "\n",
        "# Log model hyperparameters to wandb\n",
        "wandb.config.update({\n",
        "    \"model_type\": \"BiLSTM Seq2Seq with Attention\",\n",
        "    \"embedding_dim\": embedding_dim,\n",
        "    \"hidden_dim\": hidden_dim,\n",
        "    \"num_layers\": num_layers,\n",
        "    \"dropout\": dropout,\n",
        "    \"vocab_size\": len(tokenizer)\n",
        "})\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define custom data collator to handle the batch preparation\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "class CustomDataCollator(DataCollatorWithPadding):\n",
        "    def __init__(self, tokenizer, padding=True, max_length=None):\n",
        "        super().__init__(tokenizer=tokenizer, padding=padding, max_length=max_length)\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = super().__call__(features)\n",
        "        # DO NOT move tensors to device - Trainer will handle this\n",
        "        return batch\n",
        "\n",
        "# Define training arguments with wandb integration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./biLSTMS_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=20,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    report_to=\"wandb\",  # Enable wandb reporting\n",
        "    run_name=\"bilstm-seq2seq\",\n",
        "    dataloader_pin_memory=False,\n",
        ")\n",
        "\n",
        "# Custom callback to log example predictions\n",
        "class LogPredictionCallback(TrainerCallback):\n",
        "    def __init__(self, model, tokenizer, eval_dataset, num_examples=3):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.num_examples = num_examples\n",
        "\n",
        "    def on_evaluate(self, args, state, control,metrics=None, **kwargs):\n",
        "        # Get a few examples from evaluation dataset\n",
        "        indices = random.sample(range(len(self.eval_dataset)), min(self.num_examples, len(self.eval_dataset)))\n",
        "        examples = [self.eval_dataset[i] for i in indices]\n",
        "\n",
        "        for i, example in enumerate(examples):\n",
        "            input_text = self.tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
        "            reference = self.tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
        "\n",
        "            # Generate summary\n",
        "            input_ids = torch.tensor([example['input_ids']]).to(device)\n",
        "            with torch.no_grad():\n",
        "                prediction_ids = self.model.generate(input_ids, max_len=128)\n",
        "                prediction = self.tokenizer.decode(prediction_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Log to wandb\n",
        "            wandb.log({\n",
        "                f\"example_{i}/input\": wandb.Html(input_text[:500] + \"...\"),\n",
        "                f\"example_{i}/reference\": wandb.Html(reference),\n",
        "                f\"example_{i}/prediction\": wandb.Html(prediction)\n",
        "            })\n",
        "\n",
        "        return control\n",
        "\n",
        "# Initialize the early stopping callback\n",
        "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "\n",
        "# Define Trainer with callbacks\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=CustomDataCollator(tokenizer),\n",
        "    callbacks=[\n",
        "        early_stopping_callback,\n",
        "        LogPredictionCallback(base_model, tokenizer, tokenized_val)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "torch.save({\n",
        "    'model_state_dict': base_model.state_dict(),\n",
        "    'vocab_size': len(tokenizer),\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers\n",
        "}, \"biLSTMs_model.pth\")\n",
        "\n",
        "# Log model artifact to wandb\n",
        "wandb.save(\"biLSTMs_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 591,
          "referenced_widgets": [
            "f7b0c30ac1df4e9bad93499b55d38de6",
            "91691b82efe6438c8d7183d34377176f",
            "4a0004ce6fff4745a69ef06d717d6176",
            "f2b416d4c25e4280b0752c91e586a981",
            "bf14afb4c3ca421d8261ea1e286b94ca",
            "c3aed9cc89d647288d1a9487ce113cac",
            "5f0034762f3a496d94323c04d1fa11f7",
            "be0b10d0bab04a4c8f78ef0df44dcb49",
            "e02df059938945629effd5c6b2cccb04",
            "7504f55c9246475fba256e59db72d965",
            "e409d052cdcc48e28dd0b6e807783f61",
            "d6a624739adb45afbca9f8ae298a0d6f",
            "57e4b8ea46364cd4845283bf7fbeebc9",
            "2e2425cc551c451e838a09370ff01ee9",
            "e0e46f7b7d504b7c8cdb2ee86051cc0f",
            "6c9731993c1048c7b0e72b34d43106c9",
            "5e893660331641599d52157947e3cc72",
            "0fce2ac7193540e2bceb9636ee9a2a12",
            "319b69fefb284a4ab11d6bacd394ac6e",
            "385f3fa4c2f8418b96e4b1c122afbe3e",
            "db50ad442e5842c8bb27deffac895c5b",
            "bb2dab403ae84eceb7a1d69e77453fdd"
          ]
        },
        "id": "4T3eBvYFdEoN",
        "outputId": "ee3c20a3-b45d-4b94-db5b-5e15c693e335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/258 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7b0c30ac1df4e9bad93499b55d38de6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/29 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d6a624739adb45afbca9f8ae298a0d6f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='715' max='1300' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 715/1300 34:41 < 28:27, 0.34 it/s, Epoch 11/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>7.177500</td>\n",
              "      <td>5.994132</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.642500</td>\n",
              "      <td>4.882666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.357000</td>\n",
              "      <td>4.398864</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.240400</td>\n",
              "      <td>4.239155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>3.606800</td>\n",
              "      <td>4.255985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>3.950400</td>\n",
              "      <td>4.153348</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>3.836400</td>\n",
              "      <td>4.150993</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>3.506200</td>\n",
              "      <td>4.147541</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>3.651700</td>\n",
              "      <td>4.160427</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.667800</td>\n",
              "      <td>4.148197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>3.699900</td>\n",
              "      <td>4.158571</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/wandb/run-20250331_142643-ywnqerul/files/biLSTMs_model.pth']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMSummarizer:\n",
        "    def __init__(self, model_path, tokenizer, device='cuda'):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Load model configuration\n",
        "        checkpoint = torch.load(model_path, map_location=self.device)\n",
        "\n",
        "        # Initialize model with saved parameters\n",
        "        self.model = BiLSTMSeq2Seq(\n",
        "            vocab_size=checkpoint['vocab_size'],\n",
        "            embedding_dim=checkpoint['embedding_dim'],\n",
        "            hidden_dim=checkpoint['hidden_dim'],\n",
        "            num_layers=checkpoint.get('num_layers', 1)\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load weights\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate_summary(self, input_text, max_length=128):\n",
        "        \"\"\"Generate summary using BiLSTM model\"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            return_tensors='pt',\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        ).input_ids.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            summary_ids = self.model.generate(inputs, max_len=max_length)\n",
        "            return self.tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
        "\n",
        "    def evaluate(self, test_df, text_col='article', target_col='highlights'):\n",
        "        \"\"\"Evaluate BiLSTM performance using ROUGE metrics\"\"\"\n",
        "        from rouge_score import rouge_scorer\n",
        "\n",
        "        generated_summaries = []\n",
        "        reference_summaries = []\n",
        "\n",
        "        for _, row in test_df.iterrows():\n",
        "            input_text = row[text_col]\n",
        "            generated = self.generate_summary(input_text)\n",
        "            generated_summaries.append(generated)\n",
        "            reference_summaries.append(row[target_col])\n",
        "\n",
        "        return self._calculate_rouge(generated_summaries, reference_summaries)\n",
        "\n",
        "    def _calculate_rouge(self, generated, references):\n",
        "        \"\"\"Calculate ROUGE scores\"\"\"\n",
        "        from rouge_score import rouge_scorer\n",
        "\n",
        "        scorer = rouge_scorer.RougeScorer(\n",
        "            ['rouge1', 'rouge2', 'rougeL'],\n",
        "            use_stemmer=True\n",
        "        )\n",
        "\n",
        "        scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "        for gen, ref in zip(generated, references):\n",
        "            score = scorer.score(ref, gen)\n",
        "            scores['rouge1'].append(score['rouge1'].fmeasure)\n",
        "            scores['rouge2'].append(score['rouge2'].fmeasure)\n",
        "            scores['rougeL'].append(score['rougeL'].fmeasure)\n",
        "\n",
        "        return {\n",
        "            metric: sum(values)/len(values) if values else 0\n",
        "            for metric, values in scores.items()\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "id": "rZyRYVsBZeHA"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install rouge package\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'rouge-score'])\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize BiLSTM Summarizer\n",
        "bilstm_summarizer = BiLSTMSummarizer(\n",
        "    model_path=\"biLSTMs_model.pth\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Test on a sample article\n",
        "sample_article = test_df.iloc[0]['article']\n",
        "generated_summary = bilstm_summarizer.generate_summary(sample_article)\n",
        "actual_summary = test_df.iloc[0]['highlights']\n",
        "\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)\n",
        "print(\"\\nActual Summary:\")\n",
        "print(actual_summary)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_sample = test_df.head(10)\n",
        "rouge_scores = bilstm_summarizer.evaluate(test_sample)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nBiLSTM ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
        "\n",
        "# Log final evaluation metrics to wandb\n",
        "wandb.log({\n",
        "    \"final_rouge1\": rouge_scores['rouge1'],\n",
        "    \"final_rouge2\": rouge_scores['rouge2'],\n",
        "    \"final_rougeL\": rouge_scores['rougeL']\n",
        "})\n",
        "\n",
        "# Create a table for the test examples\n",
        "test_table = wandb.Table(columns=[\"Article\", \"Reference\", \"Generated\"])\n",
        "\n",
        "# Add a few examples to the table\n",
        "for i in range(min(5, len(test_sample))):\n",
        "    article = test_sample.iloc[i]['article']\n",
        "    reference = test_sample.iloc[i]['highlights']\n",
        "    generated = bilstm_summarizer.generate_summary(article)\n",
        "    test_table.add_data(article[:300] + \"...\", reference, generated)\n",
        "\n",
        "# Log the table\n",
        "wandb.log({\"test_examples\": test_table})\n",
        "\n",
        "# Finish the wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WoQy5XQUsGa",
        "outputId": "b4aa49ef-aef1-43bc-a93e-940aa6c6781e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "\n",
            "Generated Summary:\n",
            " wish suchewitness capt women to goals. of railing inHarry northern Open toiggs . .The a- four Hughes's phenomena . .he impression\n",
            " Beat . length\n",
            "\n",
            "Actual Summary:\n",
            "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
            "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
            "\n",
            "BiLSTM ROUGE Scores:\n",
            "ROUGE-1: 17.12\n",
            "ROUGE-2: 16.24\n",
            "ROUGE-L: 23.03\n",
            "\n",
            "\n",
            "Run history:\n",
            "\n",
            "eval/loss\t█▄▂▁▁▁▁▁▁▁▁\n",
            "eval/runtime\t██▇▁▇▃▇▁▇▃▅\n",
            "eval/samples_per_second\t▁▂▂█▂▆▂█▂▆▅\n",
            "eval/steps_per_second\t▁▁▂█▂▆▂█▂▆▄\n",
            "final_rouge1\t▁\n",
            "final_rouge2\t▁\n",
            "final_rougeL\t▁\n",
            "train/epoch\t▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███\n",
            "train/global_step\t▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇█████\n",
            "train/grad_norm\t▃▅▆▁▁▂▁▅▂▃▂▃▂▂▄▆▂▃▂▂▃▂▁▁█▂▅▃▂▂▁█▂▁▃▇▃▂▁▂\n",
            "train/learning_rate\t██▇▇▇▇▇▇▆▆▆▅▅▅▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁\n",
            "train/loss\t█▇▇▆▅▃▂▂▂▂▁▁▂▁▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▁▁▁▁\n",
            "\n",
            "Run summary:\n",
            "\n",
            "eval/loss\t\t4.15857\n",
            "eval/runtime\t\t4.2908\n",
            "eval/samples_per_second\t6.759\n",
            "eval/steps_per_second\t1.864\n",
            "final_rouge1\t\t17.12\n",
            "final_rouge2\t\t16.24\n",
            "final_rougeL\t\t23.03\n",
            "total_flos\t\t0.84\n",
            "train/epoch\t\t11\n",
            "train/global_step\t715\n",
            "train/grad_norm\t\t1.99132\n",
            "train/learning_rate\t1e-05\n",
            "train/loss\t\t3.6999\n",
            "train_loss\t\t4.49725\n",
            "train_runtime\t\t2083.7925\n",
            "train_samples_per_second\t2.476\n",
            "train_steps_per_second\t0.624\n",
            "\n",
            "View run bilstm-seq2seq at: https://wandb.ai/b22cs093-prom-iit-rajasthan/bilstm-summarization/runs/ywnqerul\n",
            "View project at: https://wandb.ai/b22cs093-prom-iit-rajasthan/bilstm-summarization\n",
            "Synced 5 W&B file(s), 100 media file(s), 2 artifact file(s) and 1 other file(s)\n",
            "Find logs at: ./wandb/run-20250331_142643-ywnqerul/logs\n"
          ]
        }
      ]
    }
  ]
}