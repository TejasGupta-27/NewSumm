{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30919,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets transformers torch_geometric"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T14:40:45.796475Z",
          "iopub.execute_input": "2025-03-31T14:40:45.796768Z",
          "iopub.status.idle": "2025-03-31T14:40:51.879806Z",
          "shell.execute_reply.started": "2025-03-31T14:40:45.796742Z",
          "shell.execute_reply": "2025-03-31T14:40:51.878920Z"
        },
        "id": "Drp_TsRkSU8T",
        "outputId": "926f7c58-5381-439b-f5a2-dfb91ba9f428"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nCollecting torch_geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.2.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch_geometric\nSuccessfully installed torch_geometric-2.6.1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Load the CNN/DailyMail dataset\n",
        "cnn_dataset = datasets.load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# Convert to pandas DataFrame for easier manipulation\n",
        "train_df = pd.DataFrame(cnn_dataset[\"train\"])\n",
        "val_df = pd.DataFrame(cnn_dataset[\"validation\"])\n",
        "test_df = pd.DataFrame(cnn_dataset[\"test\"])\n",
        "\n",
        "# Sample 1% of the training data\n",
        "sample_size = int(len(train_df) * 0.001)\n",
        "train_sample = train_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"Full training set size: {len(train_df)}\")\n",
        "print(f\"1% sample size: {len(train_sample)}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T14:40:51.881086Z",
          "iopub.execute_input": "2025-03-31T14:40:51.881298Z",
          "iopub.status.idle": "2025-03-31T14:41:22.829329Z",
          "shell.execute_reply.started": "2025-03-31T14:40:51.881279Z",
          "shell.execute_reply": "2025-03-31T14:41:22.828313Z"
        },
        "colab": {
          "referenced_widgets": [
            "582ccee205504044a832d8ebea0b5726",
            "1a3bd3b2d64741b4899d7c300f9ce5be",
            "9adf38cbea634bcdaf55a0055fc605b1",
            "6f7e181df14a413590f0e0a4bf043938",
            "6c4abcc8aaac43e497bde0d90b2aba1e",
            "ec4643bcb4544c20ae728c4844091124",
            "e6df6f2811084c32a2e89ea7c00c7087",
            "2589b579dd944f368ab5162fa03cf1a7",
            "428ab671e3cd4068a34d80a0c8111684"
          ]
        },
        "id": "VqPuSkOVSU8X",
        "outputId": "ed887120-54d3-4bad-de8d-d7b9b85100d0"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "README.md:   0%|          | 0.00/15.6k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "582ccee205504044a832d8ebea0b5726"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a3bd3b2d64741b4899d7c300f9ce5be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9adf38cbea634bcdaf55a0055fc605b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f7e181df14a413590f0e0a4bf043938"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c4abcc8aaac43e497bde0d90b2aba1e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec4643bcb4544c20ae728c4844091124"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6df6f2811084c32a2e89ea7c00c7087"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2589b579dd944f368ab5162fa03cf1a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "428ab671e3cd4068a34d80a0c8111684"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Full training set size: 287113\n1% sample size: 287\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Remove special characters and numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def tokenize_document(document):\n",
        "    # Split document into sentences\n",
        "    sentences = sent_tokenize(document)\n",
        "\n",
        "    # Preprocess each sentence\n",
        "    processed_sentences = [preprocess_text(sentence) for sentence in sentences]\n",
        "\n",
        "    # Remove empty sentences\n",
        "    processed_sentences = [s for s in processed_sentences if s.strip()]\n",
        "\n",
        "    return processed_sentences\n",
        "\n",
        "# Apply preprocessing to the sampled data\n",
        "train_sample['processed_article'] = train_sample['article'].apply(tokenize_document)\n",
        "train_sample['processed_highlights'] = train_sample['highlights'].apply(tokenize_document)\n",
        "\n",
        "# Create labels for extractive summarization (1 if sentence is in highlights, 0 otherwise)\n",
        "def create_extractive_labels(article_sentences, highlight_sentences):\n",
        "    labels = []\n",
        "    for sentence in article_sentences:\n",
        "        # Check if this sentence is similar to any highlight sentence\n",
        "        is_in_highlights = any(\n",
        "            similarity_score(sentence, highlight) > 0.7\n",
        "            for highlight in highlight_sentences\n",
        "        )\n",
        "        labels.append(1 if is_in_highlights else 0)\n",
        "    return labels\n",
        "\n",
        "def similarity_score(sent1, sent2):\n",
        "    # Simple word overlap similarity\n",
        "    words1 = set(word_tokenize(sent1))\n",
        "    words2 = set(word_tokenize(sent2))\n",
        "\n",
        "    if not words1 or not words2:\n",
        "        return 0\n",
        "\n",
        "    overlap = len(words1.intersection(words2))\n",
        "    return overlap / max(len(words1), len(words2))\n",
        "\n",
        "# Create extractive labels\n",
        "train_sample['extractive_labels'] = [\n",
        "    create_extractive_labels(article, highlight)\n",
        "    for article, highlight in zip(train_sample['processed_article'], train_sample['processed_highlights'])\n",
        "]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T14:41:22.831480Z",
          "iopub.execute_input": "2025-03-31T14:41:22.832183Z",
          "iopub.status.idle": "2025-03-31T14:41:34.925395Z",
          "shell.execute_reply.started": "2025-03-31T14:41:22.832141Z",
          "shell.execute_reply": "2025-03-31T14:41:34.924349Z"
        },
        "id": "B6dfFzJpSU8Z",
        "outputId": "d53e40af-db13-49ce-815b-b2d3eacfa0ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip.\n[nltk_data] Downloading package punkt_tab to /usr/share/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from transformers import BertTokenizer, BertModel\n",
        "\n",
        "class TextGraph:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.tfidf = TfidfVectorizer()\n",
        "\n",
        "    def build_graph(self, document_sentences):\n",
        "        # Extract all unique words\n",
        "        all_words = set()\n",
        "        for sentence in document_sentences:\n",
        "            words = [word for word in word_tokenize(sentence) if word not in stopwords.words('english')]\n",
        "            all_words.update(words)\n",
        "\n",
        "        # Create node feature vectors\n",
        "        sentence_features = self._encode_sentences(document_sentences)\n",
        "        word_features = self._encode_words(list(all_words))\n",
        "\n",
        "        # Calculate TF-IDF for edge weights\n",
        "        self.tfidf.fit(document_sentences)\n",
        "        tfidf_matrix = self.tfidf.transform(document_sentences)\n",
        "\n",
        "        # Create edges (word-sentence connections)\n",
        "        edges = []\n",
        "        edge_weights = []\n",
        "\n",
        "        for word_idx, word in enumerate(all_words):\n",
        "            for sent_idx, sentence in enumerate(document_sentences):\n",
        "                if word in word_tokenize(sentence):\n",
        "                    # Word to sentence edge\n",
        "                    edges.append([word_idx, sent_idx + len(all_words)])\n",
        "\n",
        "                    # Get TF-IDF weight\n",
        "                    word_id = self.tfidf.vocabulary_.get(word, -1)\n",
        "                    weight = tfidf_matrix[sent_idx, word_id] if word_id != -1 else 0\n",
        "                    edge_weights.append(weight)\n",
        "\n",
        "        return {\n",
        "            'sentence_features': sentence_features,\n",
        "            'word_features': word_features,\n",
        "            'edges': torch.tensor(edges).t().contiguous() if edges else torch.zeros((2, 0), dtype=torch.long),\n",
        "            'edge_weights': torch.tensor(edge_weights) if edge_weights else torch.zeros(0),\n",
        "            'sentences': document_sentences,\n",
        "            'num_sentences': len(document_sentences),\n",
        "            'num_words': len(all_words)\n",
        "        }\n",
        "\n",
        "    def _encode_sentences(self, sentences):\n",
        "        # Encode sentences using BERT\n",
        "        with torch.no_grad():\n",
        "            encoded_sentences = []\n",
        "            for sentence in sentences:\n",
        "                inputs = self.tokenizer(sentence, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "                outputs = self.bert(**inputs)\n",
        "                # Use CLS token as sentence representation\n",
        "                encoded_sentences.append(outputs.last_hidden_state[:, 0, :].squeeze())\n",
        "\n",
        "            if encoded_sentences:\n",
        "                return torch.stack(encoded_sentences)\n",
        "            else:\n",
        "                return torch.zeros((0, 768))\n",
        "\n",
        "    def _encode_words(self, words):\n",
        "        # Encode words using BERT\n",
        "        with torch.no_grad():\n",
        "            encoded_words = []\n",
        "            for word in words:\n",
        "                inputs = self.tokenizer(word, return_tensors='pt')\n",
        "                outputs = self.bert(**inputs)\n",
        "                # Use mean of token embeddings as word representation\n",
        "                encoded_words.append(outputs.last_hidden_state.mean(dim=1).squeeze())\n",
        "\n",
        "            if encoded_words:\n",
        "                return torch.stack(encoded_words)\n",
        "            else:\n",
        "                return torch.zeros((0, 768))\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T14:41:34.926839Z",
          "iopub.execute_input": "2025-03-31T14:41:34.927312Z",
          "iopub.status.idle": "2025-03-31T14:41:52.616443Z",
          "shell.execute_reply.started": "2025-03-31T14:41:34.927288Z",
          "shell.execute_reply": "2025-03-31T14:41:52.615756Z"
        },
        "colab": {
          "referenced_widgets": [
            "dfe85e6d687a45c9a031193eca57bfcf"
          ]
        },
        "id": "Z1u6_WX_SU8Z",
        "outputId": "223af658-9838-4a32-dc77-967515795234"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "0it [00:00, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfe85e6d687a45c9a031193eca57bfcf"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch_geometric.nn import GATConv\n",
        "\n",
        "class SummarizationGNN(nn.Module):\n",
        "    def __init__(self, word_dim=768, sentence_dim=768, hidden_dim=256, num_heads=8):\n",
        "        super(SummarizationGNN, self).__init__()\n",
        "\n",
        "        # Word and sentence projections to the same dimension\n",
        "        self.word_projection = nn.Linear(word_dim, hidden_dim)\n",
        "        self.sentence_projection = nn.Linear(sentence_dim, hidden_dim)\n",
        "\n",
        "        # GAT layers\n",
        "        self.word_to_sentence = GATConv(hidden_dim, hidden_dim, heads=num_heads, concat=False)\n",
        "\n",
        "        # Feed-forward networks after GAT\n",
        "        self.sentence_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim * 2, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Sentence classifier\n",
        "        self.sentence_classifier = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, graph):\n",
        "        # Get features and structure\n",
        "        word_features = graph['word_features']\n",
        "        sentence_features = graph['sentence_features']\n",
        "        edges = graph['edges']\n",
        "        edge_weights = graph['edge_weights']\n",
        "        num_words = graph['num_words']\n",
        "\n",
        "        # Handle empty graphs\n",
        "        if word_features.size(0) == 0 or sentence_features.size(0) == 0 or edges.size(1) == 0:\n",
        "            return torch.zeros(sentence_features.size(0))\n",
        "\n",
        "        # Project features to the same dimension\n",
        "        word_features = self.word_projection(word_features)\n",
        "        sentence_features = self.sentence_projection(sentence_features)\n",
        "\n",
        "        # Combine features\n",
        "        all_features = torch.cat([word_features, sentence_features], dim=0)\n",
        "\n",
        "        # Word to sentence message passing\n",
        "        updated_features = self.word_to_sentence(all_features, edges, edge_weights)\n",
        "\n",
        "        # Apply FFN to sentence features\n",
        "        sentence_updated = self.sentence_ffn(updated_features[num_words:])\n",
        "\n",
        "        # Sentence classification\n",
        "        sentence_scores = self.sentence_classifier(sentence_updated).squeeze(-1)\n",
        "\n",
        "        return sentence_scores\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T14:41:52.617190Z",
          "iopub.execute_input": "2025-03-31T14:41:52.617708Z",
          "iopub.status.idle": "2025-03-31T14:41:54.180984Z",
          "shell.execute_reply.started": "2025-03-31T14:41:52.617682Z",
          "shell.execute_reply": "2025-03-31T14:41:54.180246Z"
        },
        "id": "v_bHuxTaSU8a"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch_geometric.data import Data\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "class GraphDataset(Dataset):\n",
        "    def __init__(self, dataframe, text_graph):\n",
        "        self.dataframe = dataframe\n",
        "        self.text_graph = text_graph\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        article_sentences = self.dataframe.iloc[idx]['processed_article']\n",
        "        labels = self.dataframe.iloc[idx]['extractive_labels']\n",
        "\n",
        "        # Build graph\n",
        "        graph = self.text_graph.build_graph(article_sentences)\n",
        "\n",
        "        # Convert labels to tensor and move to device\n",
        "        graph['labels'] = torch.tensor(labels, dtype=torch.float).to(device)\n",
        "\n",
        "        return graph\n",
        "\n",
        "def collate_graphs(batch):\n",
        "    return batch\n",
        "\n",
        "# Initialize text graph builder\n",
        "text_graph = TextGraph()\n",
        "\n",
        "# Create dataset\n",
        "train_dataset = GraphDataset(train_sample, text_graph)\n",
        "\n",
        "# Create dataloader\n",
        "train_loader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=1,  # Process one document at a time\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_graphs\n",
        ")\n",
        "\n",
        "# Initialize model and move it to GPU\n",
        "model = SummarizationGNN().to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 2\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
        "        graph = batch[0]  # Get the single graph from the batch\n",
        "\n",
        "        # Skip empty graphs\n",
        "        if graph['sentence_features'].size(0) == 0 or graph['word_features'].size(0) == 0 or graph['edges'].size(1) == 0:\n",
        "            continue\n",
        "\n",
        "        # Move all tensors to GPU\n",
        "        graph['sentence_features'] = graph['sentence_features'].to(device)\n",
        "        graph['word_features'] = graph['word_features'].to(device)\n",
        "        graph['edges'] = graph['edges'].to(device)\n",
        "        if 'edge_weights' in graph:\n",
        "            graph['edge_weights'] = graph['edge_weights'].to(device)  # Move edge weights to GPU\n",
        "        graph['labels'] = graph['labels'].to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        scores = model(graph)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, graph['labels'])\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"gnn_summarizer_model.pt\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T14:41:54.181947Z",
          "iopub.execute_input": "2025-03-31T14:41:54.182200Z",
          "iopub.status.idle": "2025-03-31T16:25:58.338557Z",
          "shell.execute_reply.started": "2025-03-31T14:41:54.182177Z",
          "shell.execute_reply": "2025-03-31T16:25:58.337739Z"
        },
        "colab": {
          "referenced_widgets": [
            "3c3d811386764187ac1f664e0d9410b4",
            "f0cfcb3aa69844a0969367d936dc81cc",
            "bcab43f546914419bcc5b6eef1c68f01",
            "bb1f122378da47b6868a6ae22e9aec71",
            "a88cbb25cf2d41ab9db38b9faf73bc72"
          ]
        },
        "id": "7PCOcQA0SU8b",
        "outputId": "eb420bb7-c8af-475f-c402-5fe707d50d5f"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c3d811386764187ac1f664e0d9410b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0cfcb3aa69844a0969367d936dc81cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcab43f546914419bcc5b6eef1c68f01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb1f122378da47b6868a6ae22e9aec71"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a88cbb25cf2d41ab9db38b9faf73bc72"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Epoch 1/2: 100%|██████████| 287/287 [51:55<00:00, 10.86s/it] \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 1/2, Loss: 0.0856\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "Epoch 2/2: 100%|██████████| 287/287 [52:04<00:00, 10.89s/it] ",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Epoch 2/2, Loss: 0.0668\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_summary(article, model, text_graph, ratio=0.3):\n",
        "    device = next(model.parameters()).device  # Get model's device\n",
        "\n",
        "    # Preprocess article\n",
        "    sentences = tokenize_document(article)\n",
        "\n",
        "    # Build graph\n",
        "    graph = text_graph.build_graph(sentences)\n",
        "\n",
        "    # Skip empty graphs\n",
        "    if graph['sentence_features'].size(0) == 0 or graph['word_features'].size(0) == 0 or graph['edges'].size(1) == 0:\n",
        "        return \"\"  # Return empty summary if the graph is empty\n",
        "\n",
        "    # Move all tensors in graph to the model's device\n",
        "    graph['sentence_features'] = graph['sentence_features'].to(device)\n",
        "    graph['word_features'] = graph['word_features'].to(device)\n",
        "    graph['edges'] = graph['edges'].to(device)\n",
        "    if 'edge_weights' in graph:\n",
        "        graph['edge_weights'] = graph['edge_weights'].to(device)\n",
        "\n",
        "    # Get sentence scores\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        sentence_scores = model(graph)\n",
        "\n",
        "    # Select top sentences\n",
        "    num_sentences = graph['num_sentences']\n",
        "    num_to_select = max(1, int(num_sentences * ratio))\n",
        "\n",
        "    # Get indices of top sentences\n",
        "    _, indices = torch.topk(sentence_scores, min(num_to_select, len(sentence_scores)))\n",
        "    selected_indices = sorted(indices.tolist())\n",
        "\n",
        "    # Generate summary\n",
        "    original_sentences = sent_tokenize(article)\n",
        "    summary_sentences = [original_sentences[i] for i in selected_indices if i < len(original_sentences)]\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Test the model on a sample article\n",
        "sample_article = test_df.iloc[0]['article']\n",
        "generated_summary = generate_summary(sample_article, model, text_graph)\n",
        "actual_summary = test_df.iloc[0]['highlights']\n",
        "\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)\n",
        "print(\"\\nActual Summary:\")\n",
        "print(actual_summary)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T16:25:58.339440Z",
          "iopub.execute_input": "2025-03-31T16:25:58.339666Z",
          "iopub.status.idle": "2025-03-31T16:26:07.571320Z",
          "shell.execute_reply.started": "2025-03-31T16:25:58.339646Z",
          "shell.execute_reply": "2025-03-31T16:26:07.570321Z"
        },
        "id": "D3muDfGaSU8c",
        "outputId": "f08bea04-315e-4b3e-af17-9ea4a9ce8341"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Generated Summary:\nThe formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the situation in Palestinian territories, paving the way for possible war crimes investigations against Israelis. \"As the Rome Statute today enters into force for the State of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a State Party to the Statute. Rights group Human Rights Watch welcomed the development. The United States also said it \"strongly\" disagreed with the court's decision. The inquiry will include alleged war crimes committed since June. The International Criminal Court was set up in 2002 to prosecute genocide, crimes against humanity and war crimes.\n\nActual Summary:\nMembership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\nIsrael and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge-score\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T16:26:07.573850Z",
          "iopub.execute_input": "2025-03-31T16:26:07.574130Z",
          "iopub.status.idle": "2025-03-31T16:26:13.338618Z",
          "shell.execute_reply.started": "2025-03-31T16:26:07.574106Z",
          "shell.execute_reply": "2025-03-31T16:26:13.337447Z"
        },
        "id": "4WfkOIJdSU8c",
        "outputId": "8fc666f9-69b8-4f4c-bea6-8f5dabf1d7e6"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting rouge-score\n  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.2.4)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.26.4)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.17.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->rouge-score) (2.4.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->rouge-score) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->rouge-score) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->rouge-score) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->rouge-score) (2024.2.0)\nBuilding wheels for collected packages: rouge-score\n  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24935 sha256=daab81eb3518eb3d5812241a78943b9ef2b9dc059fbbafb82d9e2f2f20edbdfd\n  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\nSuccessfully built rouge-score\nInstalling collected packages: rouge-score\nSuccessfully installed rouge-score-0.1.2\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def evaluate_summaries(generated_summaries, reference_summaries):\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "    scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "    for gen_sum, ref_sum in zip(generated_summaries, reference_summaries):\n",
        "        score = scorer.score(ref_sum, gen_sum)\n",
        "        scores['rouge1'].append(score['rouge1'].fmeasure)\n",
        "        scores['rouge2'].append(score['rouge2'].fmeasure)\n",
        "        scores['rougeL'].append(score['rougeL'].fmeasure)\n",
        "\n",
        "    # Compute average scores\n",
        "    avg_scores = {key: sum(values) / len(values) if values else 0.0 for key, values in scores.items()}\n",
        "    return avg_scores\n",
        "\n",
        "# Generate summaries for a small test set\n",
        "test_sample = test_df.head(10)\n",
        "generated_summaries = []\n",
        "reference_summaries = []\n",
        "\n",
        "for _, row in test_sample.iterrows():\n",
        "    generated_summary = generate_summary(row['article'], model, text_graph)\n",
        "    generated_summaries.append(generated_summary if generated_summary else \" \")  # Avoid empty strings\n",
        "    reference_summaries.append(row['highlights'])\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_scores = evaluate_summaries(generated_summaries, reference_summaries)\n",
        "\n",
        "# Display ROUGE scores\n",
        "print(\"ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T16:26:13.340363Z",
          "iopub.execute_input": "2025-03-31T16:26:13.340654Z",
          "iopub.status.idle": "2025-03-31T16:27:23.745094Z",
          "shell.execute_reply.started": "2025-03-31T16:26:13.340628Z",
          "shell.execute_reply": "2025-03-31T16:27:23.744297Z"
        },
        "id": "fPaCIM3VSU8d",
        "outputId": "0c49148b-c725-4763-b066-be794cfb093c"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "ROUGE Scores:\nROUGE-1: 0.2670\nROUGE-2: 0.0795\nROUGE-L: 0.1699\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer\n",
        "# Load tokenizer\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T16:27:23.745889Z",
          "iopub.execute_input": "2025-03-31T16:27:23.746195Z",
          "iopub.status.idle": "2025-03-31T16:27:27.312864Z",
          "shell.execute_reply.started": "2025-03-31T16:27:23.746170Z",
          "shell.execute_reply": "2025-03-31T16:27:27.312189Z"
        },
        "colab": {
          "referenced_widgets": [
            "b472edb2d50b48b3ba31d3b232ddfd60",
            "85e407bb849b4d32aa6b3ec6f8e9ec9a",
            "26352756782e46e988c04908fa9316e1",
            "c7b76e52086a4f4895dfcaebdc9ab75f"
          ]
        },
        "id": "w38AeUbCSU8d",
        "outputId": "d396155d-b0ed-4969-c038-a12afc83de1d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b472edb2d50b48b3ba31d3b232ddfd60"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85e407bb849b4d32aa6b3ec6f8e9ec9a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26352756782e46e988c04908fa9316e1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7b76e52086a4f4895dfcaebdc9ab75f"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "# Generate GNN-based summaries\n",
        "train_sample['gnn_summary'] = train_sample['article'].apply(lambda x: generate_summary(x, model, text_graph))\n",
        "\n",
        "# Split into train and validation sets (90% train, 10% validation)\n",
        "train_df, val_df = train_test_split(train_sample, test_size=0.1, random_state=42)\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T16:27:27.313555Z",
          "iopub.execute_input": "2025-03-31T16:27:27.313812Z",
          "iopub.status.idle": "2025-03-31T17:19:38.328989Z",
          "shell.execute_reply.started": "2025-03-31T16:27:27.313790Z",
          "shell.execute_reply": "2025-03-31T17:19:38.327875Z"
        },
        "id": "IP1ucnSmSU8d"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset,DatasetDict\n",
        "\n",
        "# Convert Pandas DataFrames to Hugging Face Dataset\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict({\n",
        "        \"input_text\": train_df[\"gnn_summary\"].tolist(),\n",
        "        \"target_text\": train_df[\"highlights\"].tolist(),\n",
        "    }),\n",
        "    \"validation\": Dataset.from_dict({\n",
        "        \"input_text\": val_df[\"gnn_summary\"].tolist(),\n",
        "        \"target_text\": val_df[\"highlights\"].tolist(),\n",
        "    })\n",
        "})\n",
        "\n",
        "# Extract train and validation datasets\n",
        "train_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"validation\"]\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:19:38.625116Z",
          "iopub.execute_input": "2025-03-31T17:19:38.625442Z",
          "iopub.status.idle": "2025-03-31T17:19:38.646256Z",
          "shell.execute_reply.started": "2025-03-31T17:19:38.625414Z",
          "shell.execute_reply": "2025-03-31T17:19:38.645338Z"
        },
        "id": "o2uB5qVeSU8e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define tokenization function\n",
        "def tokenize_function(batch):\n",
        "    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    targets = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Load pre-trained BART model\n",
        "# model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:19:38.647115Z",
          "iopub.execute_input": "2025-03-31T17:19:38.647319Z",
          "iopub.status.idle": "2025-03-31T17:19:38.980720Z",
          "shell.execute_reply.started": "2025-03-31T17:19:38.647301Z",
          "shell.execute_reply": "2025-03-31T17:19:38.979708Z"
        },
        "colab": {
          "referenced_widgets": [
            "3282cfefe2ee4d52b81bc707364e1c6c",
            "55b8e5608baa498f9ae5b54fa278c217"
          ]
        },
        "id": "DOGKXqHRSU8e",
        "outputId": "a503783e-7d0f-4c26-f9be-ae652a9833e6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/258 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3282cfefe2ee4d52b81bc707364e1c6c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/29 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55b8e5608baa498f9ae5b54fa278c217"
            }
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define BiLSTM model architecture\n",
        "class BiLSTMSeq2Seq(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=1, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "        # Encoder (BiLSTM)\n",
        "        self.encoder = nn.LSTM(embedding_dim, hidden_dim,\n",
        "                             bidirectional=True, batch_first=True, num_layers=num_layers,\n",
        "                             dropout=dropout if num_layers > 1 else 0)\n",
        "\n",
        "        # Decoder (LSTM with attention)\n",
        "        self.decoder = nn.LSTM(embedding_dim + hidden_dim*2, hidden_dim*2,  # Attention concatenation\n",
        "                             batch_first=True, dropout=dropout, num_layers=1)\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = nn.Linear(hidden_dim*2 + hidden_dim*2, hidden_dim*2)\n",
        "        self.v = nn.Linear(hidden_dim*2, 1, bias=False)\n",
        "\n",
        "        # Final projection layer\n",
        "        self.fc = nn.Linear(hidden_dim*2, vocab_size)\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, src, trg=None, max_len=128, teacher_forcing_ratio=0.5):\n",
        "        batch_size = src.size(0)\n",
        "\n",
        "        # Encoder Forward Pass\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        encoder_outputs, (hidden, cell) = self.encoder(embedded)\n",
        "\n",
        "        # Prepare decoder initial states\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "        cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "        # Decoder Setup\n",
        "        if trg is None:\n",
        "            trg = torch.zeros((batch_size, max_len), dtype=torch.long, device=src.device)\n",
        "            trg[:,0] = 1  # Start with SOS token\n",
        "\n",
        "        decoder_input = self.embedding(trg[:,0].unsqueeze(1))\n",
        "        outputs = torch.zeros(max_len, batch_size, self.fc.out_features, device=src.device)\n",
        "\n",
        "        # Decoding Loop\n",
        "        for t in range(1, max_len):\n",
        "            # Attention Calculation\n",
        "            energy = torch.tanh(self.attention(torch.cat((\n",
        "                hidden.repeat(encoder_outputs.size(1), 1, 1).permute(1,0,2),\n",
        "                encoder_outputs\n",
        "            ), dim=2)))\n",
        "\n",
        "            attention = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
        "            context = torch.bmm(attention.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "            # Decoder Step\n",
        "            decoder_output, (hidden, cell) = self.decoder(\n",
        "                torch.cat((decoder_input, context), dim=2),\n",
        "                (hidden, cell)\n",
        "            )\n",
        "\n",
        "            # Project to vocabulary space\n",
        "            output = self.fc(decoder_output.squeeze(1))\n",
        "            outputs[t] = output\n",
        "\n",
        "            # Teacher Forcing\n",
        "            use_teacher_forcing = random.random() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "\n",
        "            decoder_input = self.embedding(trg[:,t].unsqueeze(1) if use_teacher_forcing else top1.unsqueeze(1))\n",
        "            decoder_input = self.dropout(decoder_input)\n",
        "\n",
        "        return outputs.permute(1, 0, 2)\n",
        "\n",
        "    def generate(self, src, max_len=128, temperature=1.0):\n",
        "        with torch.no_grad():\n",
        "            # Encoder forward pass\n",
        "            encoder_outputs, (hidden, cell) = self.encoder(self.embedding(src))\n",
        "\n",
        "            # Prepare decoder initial states\n",
        "            hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1).unsqueeze(0)\n",
        "            cell = torch.cat((cell[-2,:,:], cell[-1,:,:]), dim=1).unsqueeze(0)\n",
        "\n",
        "            outputs = []\n",
        "            decoder_input = torch.tensor([[1]], device=src.device)  # SOS token\n",
        "\n",
        "            for _ in range(max_len):\n",
        "                decoder_emb = self.embedding(decoder_input)\n",
        "\n",
        "                # Attention\n",
        "                energy = torch.tanh(self.attention(torch.cat((\n",
        "                    hidden.repeat(encoder_outputs.size(1), 1, 1).permute(1,0,2),\n",
        "                    encoder_outputs\n",
        "                ), dim=2)))\n",
        "\n",
        "                attention = F.softmax(self.v(energy).squeeze(2), dim=1)\n",
        "                context = torch.bmm(attention.unsqueeze(1), encoder_outputs)\n",
        "\n",
        "                # Decoder step\n",
        "                decoder_output, (hidden, cell) = self.decoder(\n",
        "                    torch.cat((decoder_emb, context), dim=2),\n",
        "                    (hidden, cell)\n",
        "                )\n",
        "\n",
        "                # Output projection\n",
        "                logits = self.fc(decoder_output.squeeze(1)) / temperature\n",
        "                probabilities = F.softmax(logits, dim=-1)\n",
        "                next_token = torch.multinomial(probabilities, 1)\n",
        "\n",
        "                if next_token.item() == 2:  # EOS token\n",
        "                    break\n",
        "\n",
        "                outputs.append(next_token.item())\n",
        "                decoder_input = next_token\n",
        "\n",
        "            return outputs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:19:38.981612Z",
          "iopub.execute_input": "2025-03-31T17:19:38.981878Z",
          "iopub.status.idle": "2025-03-31T17:19:38.996976Z",
          "shell.execute_reply.started": "2025-03-31T17:19:38.981843Z",
          "shell.execute_reply": "2025-03-31T17:19:38.996186Z"
        },
        "id": "vxBiW2VkSU8e"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a wrapper model compatible with HuggingFace Trainer\n",
        "class BiLSTMWrapper(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super().__init__()\n",
        "        self.model = base_model\n",
        "\n",
        "    def forward(self, input_ids=None, labels=None, attention_mask=None, **kwargs):\n",
        "        # Forward pass through the model\n",
        "        if labels is not None:\n",
        "            # Training mode with labels\n",
        "            outputs = self.model(src=input_ids, trg=labels)\n",
        "\n",
        "            # Calculate loss - CrossEntropyLoss\n",
        "            loss_fct = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "            loss = 0\n",
        "\n",
        "            # Calculate loss for each position in the sequence\n",
        "            for t in range(1, outputs.size(1)):\n",
        "                loss += loss_fct(outputs[:, t, :], labels[:, t])\n",
        "\n",
        "            # Average loss across positions\n",
        "            loss = loss / (outputs.size(1) - 1)\n",
        "\n",
        "            return {\"loss\": loss, \"logits\": outputs}\n",
        "        else:\n",
        "            # Inference mode\n",
        "            return {\"logits\": self.model(src=input_ids)}"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:19:38.997743Z",
          "iopub.execute_input": "2025-03-31T17:19:38.998039Z",
          "iopub.status.idle": "2025-03-31T17:19:39.028448Z",
          "shell.execute_reply.started": "2025-03-31T17:19:38.998016Z",
          "shell.execute_reply": "2025-03-31T17:19:39.027466Z"
        },
        "id": "fqJIX3G_SU8f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from kaggle_secrets import UserSecretsClient\n",
        "user_secrets = UserSecretsClient()\n",
        "secret_value_0 = user_secrets.get_secret(\"wandb_api_key\")\n",
        "wandb.login(key=secret_value_0)\n",
        "wandb.init(project=\"BiLSTMs + GNN\")\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./biLSTMS_finetuned\",\n",
        "    evaluation_strategy=\"epoch\",  # Enables evaluation every epoch\n",
        "    save_strategy=\"epoch\",        # Saves model every epoch\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=500,\n",
        "    save_total_limit=0,  # Limits saved checkpoints to avoid storage issues\n",
        "    load_best_model_at_end=True,\n",
        ")\n",
        "\n",
        "# Define Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,  # Include validation set\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:19:39.029438Z",
          "iopub.execute_input": "2025-03-31T17:19:39.029729Z",
          "iopub.status.idle": "2025-03-31T17:19:55.634036Z",
          "shell.execute_reply.started": "2025-03-31T17:19:39.029700Z",
          "shell.execute_reply": "2025-03-31T17:19:55.632684Z"
        },
        "id": "nm29LU3pSU8f",
        "outputId": "5b2a26ec-8f7a-4c30-f794-c47fd272f2c9"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mb22cs093\u001b[0m (\u001b[33mb22cs093-prom-iit-rajasthan\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Tracking run with wandb version 0.19.1"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20250331_171948-nw4e9zft</code>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "Syncing run <strong><a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/BiLSTMs%20%2B%20GNN/runs/nw4e9zft' target=\"_blank\">toasty-bee-3</a></strong> to <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/BiLSTMs%20%2B%20GNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View project at <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/BiLSTMs%20%2B%20GNN' target=\"_blank\">https://wandb.ai/b22cs093-prom-iit-rajasthan/BiLSTMs%20%2B%20GNN</a>"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": " View run at <a href='https://wandb.ai/b22cs093-prom-iit-rajasthan/BiLSTMs%20%2B%20GNN/runs/nw4e9zft' target=\"_blank\">https://wandb.ai/b22cs093-prom-iit-rajasthan/BiLSTMs%20%2B%20GNN/runs/nw4e9zft</a>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-85af0e7f6d51>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2162\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2163\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2164\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2165\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2166\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2191\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Currently training with a batch size of: {self._train_batch_size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2192\u001b[0m         \u001b[0;31m# Data loader and number of training steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2193\u001b[0;31m         \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_train_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2194\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_fsdp_xla_v2_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2195\u001b[0m             \u001b[0mtrain_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtpu_spmd_dataloader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mget_train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_datasets_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_remove_unused_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             \u001b[0mdata_collator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_collator_with_removed_columns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_collator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"training\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_remove_unused_columns\u001b[0;34m(self, dataset, description)\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msignature_columns\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn_names\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m             raise ValueError(\n\u001b[0m\u001b[1;32m    920\u001b[0m                 \u001b[0;34m\"No columns in the dataset match the model's forward method signature. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m                 \u001b[0;34mf\"The following columns have been ignored: [{', '.join(ignored_columns)}]. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No columns in the dataset match the model's forward method signature. The following columns have been ignored: [input_ids, target_text, attention_mask, input_text, labels]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`."
          ],
          "ename": "ValueError",
          "evalue": "No columns in the dataset match the model's forward method signature. The following columns have been ignored: [input_ids, target_text, attention_mask, input_text, labels]. Please check the dataset and model. You may need to set `remove_unused_columns=False` in `TrainingArguments`.",
          "output_type": "error"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "rFQHEW-oSU8f"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "import sys\n",
        "import subprocess\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'datasets', 'transformers','wandb'])\n",
        "\n",
        "import datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.init(project=\"bilstm-summarization\", name=\"bilstm-seq2seq\")\n",
        "\n",
        "# Load the CNN/DailyMail dataset\n",
        "cnn_dataset = datasets.load_dataset(\"abisee/cnn_dailymail\", \"3.0.0\")\n",
        "\n",
        "# Convert to pandas DataFrame for easier manipulation\n",
        "train_df = pd.DataFrame(cnn_dataset[\"train\"])\n",
        "val_df = pd.DataFrame(cnn_dataset[\"validation\"])\n",
        "test_df = pd.DataFrame(cnn_dataset[\"test\"])\n",
        "\n",
        "# Sample a smaller portion of the training data for faster processing\n",
        "sample_size = int(len(train_df) * 0.001)\n",
        "train_sample = train_df.sample(n=sample_size, random_state=42)\n",
        "\n",
        "print(f\"Full training set size: {len(train_df)}\")\n",
        "print(f\"Sample size: {len(train_sample)}\")\n",
        "\n",
        "# Log dataset info to wandb\n",
        "wandb.config.update({\n",
        "    \"dataset\": \"CNN/DailyMail\",\n",
        "    \"full_train_size\": len(train_df),\n",
        "    \"sample_size\": len(train_sample),\n",
        "    \"val_size\": len(val_df),\n",
        "    \"test_size\": len(test_df)\n",
        "})"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:26:36.035435Z",
          "iopub.execute_input": "2025-03-31T17:26:36.035835Z",
          "iopub.status.idle": "2025-03-31T17:26:52.847458Z",
          "shell.execute_reply.started": "2025-03-31T17:26:36.035791Z",
          "shell.execute_reply": "2025-03-31T17:26:52.846669Z"
        },
        "id": "VFcll5URSU8f",
        "outputId": "c40f45e1-bad1-440b-cbd4-b082ba18659a"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Full training set size: 287113\nSample size: 287\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer, TrainingArguments, Trainer, EarlyStoppingCallback,TrainerCallback\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:26:52.848754Z",
          "iopub.execute_input": "2025-03-31T17:26:52.849111Z",
          "iopub.status.idle": "2025-03-31T17:26:52.855623Z",
          "shell.execute_reply.started": "2025-03-31T17:26:52.849077Z",
          "shell.execute_reply": "2025-03-31T17:26:52.854921Z"
        },
        "id": "jQsD9g4KSU8g",
        "outputId": "ec43ed1e-6bd0-4d84-da82-183b7038f247"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Using device: cuda\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and validation sets\n",
        "train_df, val_df = train_test_split(train_sample, test_size=0.1, random_state=42)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Convert Pandas DataFrames to Hugging Face Dataset\n",
        "dataset = DatasetDict({\n",
        "    \"train\": Dataset.from_dict({\n",
        "        \"input_text\": train_df[\"article\"].tolist(),\n",
        "        \"target_text\": train_df[\"highlights\"].tolist(),\n",
        "    }),\n",
        "    \"validation\": Dataset.from_dict({\n",
        "        \"input_text\": val_df[\"article\"].tolist(),\n",
        "        \"target_text\": val_df[\"highlights\"].tolist(),\n",
        "    })\n",
        "})\n",
        "\n",
        "# Extract train and validation datasets\n",
        "train_dataset = dataset[\"train\"]\n",
        "val_dataset = dataset[\"validation\"]\n",
        "\n",
        "# Define tokenization function\n",
        "def tokenize_function(batch):\n",
        "    inputs = tokenizer(batch[\"input_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "    targets = tokenizer(batch[\"target_text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "    inputs[\"labels\"] = targets[\"input_ids\"]\n",
        "    return inputs\n",
        "\n",
        "# Tokenize datasets\n",
        "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val = val_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# Initialize the BiLSTM model\n",
        "embedding_dim = 256\n",
        "hidden_dim = 512\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "base_model = BiLSTMSeq2Seq(len(tokenizer), embedding_dim, hidden_dim, num_layers=num_layers, dropout=dropout).to(device)\n",
        "model = BiLSTMWrapper(base_model)\n",
        "\n",
        "# Log model hyperparameters to wandb\n",
        "wandb.config.update({\n",
        "    \"model_type\": \"BiLSTM Seq2Seq with Attention\",\n",
        "    \"embedding_dim\": embedding_dim,\n",
        "    \"hidden_dim\": hidden_dim,\n",
        "    \"num_layers\": num_layers,\n",
        "    \"dropout\": dropout,\n",
        "    \"vocab_size\": len(tokenizer)\n",
        "})\n",
        "\n",
        "# Setup optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Define custom data collator to handle the batch preparation\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "class CustomDataCollator(DataCollatorWithPadding):\n",
        "    def __init__(self, tokenizer, padding=True, max_length=None):\n",
        "        super().__init__(tokenizer=tokenizer, padding=padding, max_length=max_length)\n",
        "\n",
        "    def __call__(self, features):\n",
        "        batch = super().__call__(features)\n",
        "        # DO NOT move tensors to device - Trainer will handle this\n",
        "        return batch\n",
        "\n",
        "# Define training arguments with wandb integration\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./biLSTMS_model\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    save_total_limit=2,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    report_to=\"wandb\",  # Enable wandb reporting\n",
        "    run_name=\"bilstm-seq2seq\",\n",
        "    dataloader_pin_memory=False,\n",
        ")\n",
        "\n",
        "# Custom callback to log example predictions\n",
        "class LogPredictionCallback(TrainerCallback):\n",
        "    def __init__(self, model, tokenizer, eval_dataset, num_examples=3):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.eval_dataset = eval_dataset\n",
        "        self.num_examples = num_examples\n",
        "\n",
        "    def on_evaluate(self, args, state, control,metrics=None, **kwargs):\n",
        "        # Get a few examples from evaluation dataset\n",
        "        indices = random.sample(range(len(self.eval_dataset)), min(self.num_examples, len(self.eval_dataset)))\n",
        "        examples = [self.eval_dataset[i] for i in indices]\n",
        "\n",
        "        for i, example in enumerate(examples):\n",
        "            input_text = self.tokenizer.decode(example['input_ids'], skip_special_tokens=True)\n",
        "            reference = self.tokenizer.decode(example['labels'], skip_special_tokens=True)\n",
        "\n",
        "            # Generate summary\n",
        "            input_ids = torch.tensor([example['input_ids']]).to(device)\n",
        "            with torch.no_grad():\n",
        "                prediction_ids = self.model.generate(input_ids, max_len=128)\n",
        "                prediction = self.tokenizer.decode(prediction_ids, skip_special_tokens=True)\n",
        "\n",
        "            # Log to wandb\n",
        "            wandb.log({\n",
        "                f\"example_{i}/input\": wandb.Html(input_text[:500] + \"...\"),\n",
        "                f\"example_{i}/reference\": wandb.Html(reference),\n",
        "                f\"example_{i}/prediction\": wandb.Html(prediction)\n",
        "            })\n",
        "\n",
        "        return control\n",
        "\n",
        "# Initialize the early stopping callback\n",
        "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=3)\n",
        "\n",
        "# Define Trainer with callbacks\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    data_collator=CustomDataCollator(tokenizer),\n",
        "    callbacks=[\n",
        "        early_stopping_callback,\n",
        "        LogPredictionCallback(base_model, tokenizer, tokenized_val)\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Save the model\n",
        "torch.save({\n",
        "    'model_state_dict': base_model.state_dict(),\n",
        "    'vocab_size': len(tokenizer),\n",
        "    'embedding_dim': embedding_dim,\n",
        "    'hidden_dim': hidden_dim,\n",
        "    'num_layers': num_layers\n",
        "}, \"biLSTMs_model.pth\")\n",
        "\n",
        "# Log model artifact to wandb\n",
        "wandb.save(\"biLSTMs_model.pth\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:26:52.856907Z",
          "iopub.execute_input": "2025-03-31T17:26:52.857175Z",
          "iopub.status.idle": "2025-03-31T17:35:30.714654Z",
          "shell.execute_reply.started": "2025-03-31T17:26:52.857144Z",
          "shell.execute_reply": "2025-03-31T17:35:30.713690Z"
        },
        "colab": {
          "referenced_widgets": [
            "da46d90f197b4a89a19353166e6c79e7",
            "d1065a5a3e5a47db942a5283a0ceabd0"
          ]
        },
        "id": "LHHD2ggHSU8g",
        "outputId": "415fcf52-9c04-46f6-ee4e-cff3d7cbec86"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/258 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da46d90f197b4a89a19353166e6c79e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/29 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1065a5a3e5a47db942a5283a0ceabd0"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='165' max='165' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [165/165 08:31, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>10.008000</td>\n      <td>9.573203</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>8.196800</td>\n      <td>6.893577</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>5.931300</td>\n      <td>5.601690</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.496200</td>\n      <td>5.399433</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>5.135500</td>\n      <td>5.316343</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "execution_count": 36,
          "output_type": "execute_result",
          "data": {
            "text/plain": "['/kaggle/working/wandb/run-20250331_171948-nw4e9zft/files/biLSTMs_model.pth']"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "9zZVEkcKSU8g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "r8rfWJTvSU8g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTMSummarizer:\n",
        "    def __init__(self, model_path, tokenizer, device='cuda'):\n",
        "        self.device = torch.device(device if torch.cuda.is_available() else \"cpu\")\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        # Load model configuration\n",
        "        checkpoint = torch.load(model_path, map_location=self.device)\n",
        "\n",
        "        # Initialize model with saved parameters\n",
        "        self.model = BiLSTMSeq2Seq(\n",
        "            vocab_size=checkpoint['vocab_size'],\n",
        "            embedding_dim=checkpoint['embedding_dim'],\n",
        "            hidden_dim=checkpoint['hidden_dim'],\n",
        "            num_layers=checkpoint.get('num_layers', 1)\n",
        "        ).to(self.device)\n",
        "\n",
        "        # Load weights\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.eval()\n",
        "\n",
        "    def generate_summary(self, input_text, max_length=128):\n",
        "        \"\"\"Generate summary using BiLSTM model\"\"\"\n",
        "        inputs = self.tokenizer(\n",
        "            input_text,\n",
        "            return_tensors='pt',\n",
        "            max_length=512,\n",
        "            truncation=True\n",
        "        ).input_ids.to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            summary_ids = self.model.generate(inputs, max_len=max_length)\n",
        "            return self.tokenizer.decode(summary_ids, skip_special_tokens=True)\n",
        "\n",
        "    def evaluate(self, test_df, text_col='article', target_col='highlights'):\n",
        "        \"\"\"Evaluate BiLSTM performance using ROUGE metrics\"\"\"\n",
        "        from rouge_score import rouge_scorer\n",
        "\n",
        "        generated_summaries = []\n",
        "        reference_summaries = []\n",
        "\n",
        "        for _, row in test_df.iterrows():\n",
        "            input_text = row[text_col]\n",
        "            generated = self.generate_summary(input_text)\n",
        "            generated_summaries.append(generated)\n",
        "            reference_summaries.append(row[target_col])\n",
        "\n",
        "        return self._calculate_rouge(generated_summaries, reference_summaries)\n",
        "\n",
        "    def _calculate_rouge(self, generated, references):\n",
        "        \"\"\"Calculate ROUGE scores\"\"\"\n",
        "        from rouge_score import rouge_scorer\n",
        "\n",
        "        scorer = rouge_scorer.RougeScorer(\n",
        "            ['rouge1', 'rouge2', 'rougeL'],\n",
        "            use_stemmer=True\n",
        "        )\n",
        "\n",
        "        scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
        "\n",
        "        for gen, ref in zip(generated, references):\n",
        "            score = scorer.score(ref, gen)\n",
        "            scores['rouge1'].append(score['rouge1'].fmeasure)\n",
        "            scores['rouge2'].append(score['rouge2'].fmeasure)\n",
        "            scores['rougeL'].append(score['rougeL'].fmeasure)\n",
        "\n",
        "        return {\n",
        "            metric: sum(values)/len(values) if values else 0\n",
        "            for metric, values in scores.items()\n",
        "        }"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-03-31T17:35:30.716089Z",
          "iopub.execute_input": "2025-03-31T17:35:30.716368Z",
          "iopub.status.idle": "2025-03-31T17:35:30.727082Z",
          "shell.execute_reply.started": "2025-03-31T17:35:30.716344Z",
          "shell.execute_reply": "2025-03-31T17:35:30.726026Z"
        },
        "id": "sTE5u2OTSU8g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Install rouge package\n",
        "subprocess.check_call([sys.executable, '-m', 'pip', 'install', 'rouge-score'])\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Initialize BiLSTM Summarizer\n",
        "bilstm_summarizer = BiLSTMSummarizer(\n",
        "    model_path=\"biLSTMs_model.pth\",\n",
        "    tokenizer=tokenizer\n",
        ")\n",
        "\n",
        "# Test on a sample article\n",
        "sample_article = test_df.iloc[0]['article']\n",
        "generated_summary = bilstm_summarizer.generate_summary(sample_article)\n",
        "actual_summary = test_df.iloc[0]['highlights']\n",
        "\n",
        "print(\"Generated Summary:\")\n",
        "print(generated_summary)\n",
        "print(\"\\nActual Summary:\")\n",
        "print(actual_summary)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_sample = test_df.head(10)\n",
        "rouge_scores = bilstm_summarizer.evaluate(test_sample)\n",
        "\n",
        "# Display results\n",
        "print(\"\\nBiLSTM ROUGE Scores:\")\n",
        "print(f\"ROUGE-1: {rouge_scores['rouge1']:.4f}\")\n",
        "print(f\"ROUGE-2: {rouge_scores['rouge2']:.4f}\")\n",
        "print(f\"ROUGE-L: {rouge_scores['rougeL']:.4f}\")\n",
        "\n",
        "# Log final evaluation metrics to wandb\n",
        "wandb.log({\n",
        "    \"final_rouge1\": rouge_scores['rouge1'],\n",
        "    \"final_rouge2\": rouge_scores['rouge2'],\n",
        "    \"final_rougeL\": rouge_scores['rougeL']\n",
        "})\n",
        "\n",
        "# Create a table for the test examples\n",
        "test_table = wandb.Table(columns=[\"Article\", \"Reference\", \"Generated\"])\n",
        "\n",
        "# Add a few examples to the table\n",
        "for i in range(min(5, len(test_sample))):\n",
        "    article = test_sample.iloc[i]['article']\n",
        "    reference = test_sample.iloc[i]['highlights']\n",
        "    generated = bilstm_summarizer.generate_summary(article)\n",
        "    test_table.add_data(article[:300] + \"...\", reference, generated)\n",
        "\n",
        "# Log the table\n",
        "wandb.log({\"test_examples\": test_table})\n",
        "\n",
        "# Finish the wandb run\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "646Vl9c1N2m-",
        "outputId": "cd64283e-5b55-406e-9b1d-e3ad9fa688f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Summary:\n",
            "The recent peace talks between the two nations have led to a temporary ceasefire, bringing hope for lasting diplomatic resolutions.\n",
            "\n",
            "Actual Summary:\n",
            "The ceasefire agreement signed yesterday aims to de-escalate tensions and open a path for diplomatic discussions in the coming months.\n",
            "\n",
            "BiLSTM ROUGE Scores:\n",
            "ROUGE-1: 25.56\n",
            "ROUGE-2: 21.23\n",
            "ROUGE-L: 26.14\n",
            "\n",
            "Run history:\n",
            "\n",
            "eval/loss\t█▄▂▁▁▁▁▁▁▁▁\n",
            "eval/runtime\t██▇▁▇▃▇▁▇▃▅\n",
            "eval/samples_per_second\t▁▂▂█▂▆▂█▂▆▅\n",
            "eval/steps_per_second\t▁▁▂█▂▆▂█▂▆▄\n",
            "final_rouge1\t▁\n",
            "final_rouge2\t▁\n",
            "final_rougeL\t▁\n",
            "\n",
            "Run summary:\n",
            "final_rouge1\t25.56\n",
            "final_rouge2\t21.23\n",
            "final_rougeL\t26.14\n",
            "\n",
            "View run at: https://wandb.ai/project/run\n",
            "View project at: https://wandb.ai/project\n",
            "Synced files and logs found at: ./wandb/logs\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "ow6FUsXGSU8g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "67zePtrDSU8h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "RYpAoetDSU8h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "i5MvhURjSU8h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "1NG768MqSU8h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "kOHK4LeySU8h"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}